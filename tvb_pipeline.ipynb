{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TVB PIPELINE: constructing **Brain Network Models** from empirical MRI data\n",
    " \n",
    "## The pipeline will generate structural connectomes, region-average fMRI time series and functional connectomes for simulation within The Virtual Brain\n",
    "\n",
    "### This pipeline is based on the Apps\n",
    "* thevirtualbrain/tvb-pipeline-sc:1.0 (dwMRI preprocessing, tractography)\n",
    "* thevirtualbrain/tvb-pipeline-fmriprep:1.0 (fMRI preprocessing)\n",
    "* thevirtualbrain/tvb_converter (structural connectome, region-wise fMRI, functional connectivity, TVB input data set)\n",
    "\n",
    "### In this tutorial you will learn how to ...\n",
    "* ...upload a BIDS data set to EBrains Collab 2.0\n",
    "* ...import it into a Jupyter notebook\n",
    "* ...upload it to a supercomputer via PyUnicore\n",
    "* ...create and execute batch job scripts for the supercomputer that execute the pipeline\n",
    "* ...download the results to the notebook\n",
    "* ...from there to the Storage and finally to your computer.\n",
    "\n",
    "### Authors / Feedback \n",
    "michael.schirner@charite.de  \n",
    "petra.ritter@charite.de  \n",
    "\n",
    "### Acknowledgments\n",
    "Thank you to Paul Triebkorn for co-developing the original version of this script on Collab 1\n",
    "\n",
    "## 1. Create EBRAINS Collab and upload BIDS data set\n",
    "\n",
    "1. Input data **must** be in BIDS format to run the following operations. Check out the BIDS format [here](https://bids.neuroimaging.io/) for more info. Hint: there are programs that transform data from other formats into BIDS.\n",
    "2. Navigate to the [Collabs](https://wiki.ebrains.eu/bin/view/Collabs/) page\n",
    "3. Click on the \"Create a Collab\" button and fill out the form.\n",
    "4. Navigate to the Drive in the leftmost menu. You might need to wait for a few seconds and refresh the page before it is visible.\n",
    "5. Create one folder for your notebooks and one for your data. In the following we will assume that you called the folders \"notebooks\" and \"data\".\n",
    "6. Download this Ipython notebook to your local file system and upload it into your newly created folder \"notebooks\" in your Collab.\n",
    "7. Upload your BIDS data set as a zip file into \"data\".\n",
    "8. Make sure that the two folders were successfully created and that the pipeline notebook and your input data were successfully uploaded into these folders.\n",
    "\n",
    "In this example we uploaded the file\n",
    "```\n",
    "dataset.zip\n",
    "```\n",
    "\n",
    "into the folder\n",
    "```\n",
    "Collabs / TVB PIPELINE / data\n",
    "\n",
    "```\n",
    "\n",
    "Depending on whether you used a private or public repository the file will end up in either of the following folders in the filesystem of the EBRAINS Jupyter Hub at https://lab.ebrains.eu/\n",
    " \n",
    "public_drive = 'drive/Shared with all'   \n",
    "\n",
    "private_drive = 'drive/Shared with groups'\n",
    "\n",
    "\n",
    "\n",
    "## 2. Access Collab drive from IPython notebook\n",
    "\n",
    "To read a file from a  Collab's drive, build the path like in the example below. Note that we specify two paths:\n",
    "* one to the BIDS MRI input data set and\n",
    "* one to the FreeSurfer license file, which we need to run fmriprep. If you do not have already a license file, you can obtain it following the instructions here: https://surfer.nmr.mgh.harvard.edu/fswiki/License\n",
    "\n",
    "To upload the data into your drive go to the main Collab site and click on \"Drive\" at the top of the left sidebar menu. Note that you must be logged in and you must be the creator/owner of the Collab to be able to do that. Once you are in your Collab's drive, use the buttons to create the folders 'data' and 'FreeSurfer_license' (or name them however you want, just make sure the names are correct when we build the path below) and upload the zipped BIDS data set into the former and the license.txt from FreeSurfer into the latter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/app-root/src/drive/Shared with groups/TVB PIPELINE/data/dataset.zip\n",
      "/opt/app-root/src/drive/Shared with groups/TVB PIPELINE/FreeSurfer_license/license.txt\n"
     ]
    }
   ],
   "source": [
    "# get path of home folder\n",
    "import os\n",
    "home = os.getenv('HOME')\n",
    "\n",
    "# paths for public and private drives\n",
    "public_drive = 'drive/Shared with all'\n",
    "private_drive = 'drive/Shared with groups'\n",
    "\n",
    "which_drive = private_drive\n",
    "\n",
    "# Collab name\n",
    "collab = 'TVB PIPELINE'\n",
    "\n",
    "# data folder name\n",
    "path = 'data'\n",
    "\n",
    "# filename\n",
    "dataset = 'dataset.zip'\n",
    "\n",
    "# full path\n",
    "full_path = os.path.join(home, which_drive, collab, path, dataset)\n",
    "print(full_path)\n",
    "\n",
    "# Freesurfer license file folder\n",
    "fs_path = 'FreeSurfer_license'\n",
    "full_fs_path = os.path.join(home, which_drive, collab, fs_path, 'license.txt')\n",
    "print(full_fs_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check whether the file is really there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(os.path.exists(full_path))\n",
    "print(os.path.exists(full_fs_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the above command returns false something went wrong. Note: you can use Bash commands like `ls` and `cd` to navigate in the file system and look for the file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to upload our brain model to the supercomputer. Therefore, we create a PyUnicore client.\n",
    "\n",
    "## 3. Upload BIDS data set to supercomputer\n",
    "First, we update PyUnicore, if necessary. Then, we import it. Finally, we connect with Piz Daint. To see which other supercomputers are there, and to learn their ID run the commented \n",
    "```\n",
    "r.site_urls\n",
    "```\n",
    "\n",
    "To select a different supercomputer replace the supercomputer identifier string in\n",
    "```\n",
    "site_client = r.site('DAINT-CSCS')\n",
    "```\n",
    "with your preferred supercomputer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: pyunicore in /opt/app-root/lib/python3.6/site-packages (0.5.9)\n",
      "Requirement already satisfied, skipping upgrade: PyJWT>=1.7 in /opt/app-root/lib/python3.6/site-packages (from pyunicore) (1.7.1)\n",
      "Requirement already satisfied, skipping upgrade: requests>=2.5 in /opt/app-root/lib/python3.6/site-packages (from pyunicore) (2.23.0)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/app-root/lib/python3.6/site-packages (from requests>=2.5->pyunicore) (1.25.8)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /opt/app-root/lib/python3.6/site-packages (from requests>=2.5->pyunicore) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/app-root/lib/python3.6/site-packages (from requests>=2.5->pyunicore) (2019.11.28)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /opt/app-root/lib/python3.6/site-packages (from requests>=2.5->pyunicore) (2.9)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/opt/app-root/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# use the pyunicore library\n",
    "!pip install pyunicore --upgrade\n",
    "import pyunicore.client as unicore_client\n",
    "\n",
    "tr = unicore_client.Transport(clb_oauth.get_token())\n",
    "r = unicore_client.Registry(tr, unicore_client._HBP_REGISTRY_URL)\n",
    "site_client = r.site('DAINT-CSCS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we start an \"empty\" interactive job to get a workspace on Piz Daint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_description = {}\n",
    "job = site_client.new_job(job_description)\n",
    "storage = job.working_dir\n",
    "#storage.properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's check the contents of the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "storage.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good, it's empty. If it weren't empty we can remove files or folders with `storage.rm(filename)` or `storage.rmdir(foldername)`. Run `help(storage)` for more information.\n",
    "\n",
    "Before uploading the BIDS ZIP file, let's look in which folder we landed on the supercomputer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/scratch/snx3000/unicore/FILESPACE/8728b479-3392-4a2f-a984-e64ba09ca627/'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "working_dir = (storage.properties['mountPoint']).encode('ascii')\n",
    "working_dir = working_dir.decode('utf-8') # we get a \"byte\"-type but need a string type\n",
    "working_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good. We will need the path to the working directory later.\n",
    "\n",
    "Now, let's copy the ZIP file to the supercomputer and check whether it arrived."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset.zip': PathFile: dataset.zip}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "storage.upload(input_name = full_path, destination = dataset)\n",
    "storage.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dataset.zip` is there -- the upload was successful.\n",
    "Now we need to extract the ZIP file. We will use the program `unzip` for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use this client to execute the unzip command and take a quick look into the folder whether `unzip` started working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec_result = site_client.execute(\"unzip \" + working_dir + dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The execute command launches unzip and forwards the result into a new folder. Let's find out the path of the folder..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/scratch/snx3000/unicore/FILESPACE/2dc283ff-6984-44f3-95b8-2ab7bd7bb44f/'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_folder = exec_result.working_dir.properties['mountPoint'].encode('ascii')\n",
    "wd_handle = exec_result.working_dir\n",
    "base_folder = base_folder.decode('utf-8')\n",
    "base_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and its contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'UNICORE_SCRIPT_EXIT_CODE': PathFile: UNICORE_SCRIPT_EXIT_CODE,\n",
       " 'UNICORE_SCRIPT_PID': PathFile: UNICORE_SCRIPT_PID,\n",
       " 'stderr': PathFile: stderr,\n",
       " 'dataset/': PathDir: dataset/,\n",
       " 'stdout': PathFile: stdout,\n",
       " '__MACOSX/': PathDir: __MACOSX/}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exec_result.working_dir.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Unzip` worked, the folder `dataset` has been created (along with some UNICORE related output files and everything written to stdout/stderr during the command).  \n",
    "\n",
    "## 4. Define environment variables\n",
    "\n",
    "We have a working directory on the supercomputer and our data is there. Let's define some environment variables with important folder paths that we can use to call the BIDS Apps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvb_output = \"tvb_converter_workdir\"# output folder name\n",
    "\n",
    "input_dir = base_folder + \"dataset\" # the name of the folder extracted from the ZIP file\n",
    "output_dir = base_folder + tvb_output # full path to output folder\n",
    "mrtrix_output = output_dir + \"/mrtrix_output\"\n",
    "\n",
    "fmriprep_output = output_dir + \"/fmriprep_output\"\n",
    "fmriprep_workdir = fmriprep_output + \"/tmp\"\n",
    "tvb_output = output_dir + \"/TVB_output\"\n",
    "tvb_workdir = tvb_output + \"/tmp\"\n",
    "\n",
    "participant_label = \"CON03\" # participant_label of BIDS dataset\n",
    "parcellation = \"desikan\" # parcellation atlas for SC and FC -- check out MRtrix3_connectome github page for available options\n",
    "n_cpus = \"36\" # how many CPUs does your HPC node have? We'll set the number of parallel threads accordingly\n",
    "\n",
    "task_name=\"ArchiSocial\" # name of task fmri, as specified in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run BIDS App `mrtrix3_connectome` to generate a structural connectome\n",
    "\n",
    "Here we create a SLURM batch job script for launching the `mtrix3_connectome` BIDS App. Instead of directly starting a job on the batch system, we use PyUnicore to submit a job on the login node, which in turn submits a job for the batch system. This gives us a greater flexibility to configure our job, we don't have to learn so much PyUnicore (although it's great!) and are failsafe if PyUnicore misses bindings for certain job managers. Note that before we run the container, we make sure that the image is up to date, or, if non-existent, gets pulled for the first time.\n",
    "\n",
    "Below is a brief outline of the `sarus run` command. For a great in-depth tutorial check out the help pages of the Swiss CSCS supercomputing site: https://user.cscs.ch/tools/containers/\n",
    "\n",
    "sarus run <container_name>\n",
    "is the standard way of running a container. Here, we additionally use the mount command to directly mount the input and the output folders into the container's filesystem's top-level directories /input and /output.\n",
    "\n",
    "For in-depth instructions for the BIDS App `mrtrix3_connectome` please refer to its help page at Docker hub:  \n",
    "https://hub.docker.com/r/bids/mrtrix3_connectome/\n",
    "\n",
    "\n",
    "For an in-depth discussion of Sarus usage, check out this documentation:  \n",
    "https://user.cscs.ch/tools/containers/sarus/\n",
    "\n",
    "# Please note: There seems to be a strange bug in Sarus: sometimes the pull command   \n",
    "\n",
    "# `srun -C mc sarus pull thevirtualbrain/tvb-pipeline-sc:1.0`   \n",
    "\n",
    "# works and sometimes not. It's often easier to just log into your supercomputing account via SSH and run the pull command in the shell. Once the image with the correct tag was pulled it doesn't need to be re-pulled. Sometimes the pull works if the command was issued multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/app-root/src/drive/Shared with groups/TVB PIPELINE/data/job_script\n"
     ]
    }
   ],
   "source": [
    "# ADJUSTABLE PARAMETERS\n",
    "################################################\n",
    "wall_time = \"23:59:00\" # ADJUST wall time of job\n",
    "job_script = \"job_script\"\n",
    "################################################\n",
    "\n",
    "job_script_path = os.path.join(home, which_drive, collab, path, job_script)\n",
    "print(job_script_path)\n",
    "\n",
    "# create job_script with bash commands\n",
    "# this script will get forwarded to the supercomputer and there run the pipeline\n",
    "with open(job_script_path, \"w\") as f:\n",
    "    f.write(\"#!/bin/bash -l\\n\") \n",
    "    f.write(\"#SBATCH --time=\" + wall_time + \"\\n\")\n",
    "    f.write(\"#SBATCH --output=slurm-\" + job_script + \".out\\n\")\n",
    "    f.write(\"#SBATCH --nodes=1\\n\")\n",
    "    f.write(\"#SBATCH --ntasks-per-core=1\\n\")    \n",
    "    f.write(\"#SBATCH --ntasks-per-node=1\\n\")\n",
    "    f.write(\"#SBATCH --cpus-per-task=\" + n_cpus + \"\\n\")\n",
    "    f.write(\"#SBATCH --partition=normal\\n\")\n",
    "    f.write(\"#SBATCH --constraint=mc\\n\")\n",
    "    f.write(\"#SBATCH --hint=nomultithread\\n\") # disable hyperthreading such that all cores become available for multithreading\n",
    "    f.write(\"export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\\n\")\n",
    "    f.write(\"module load /apps/daint/UES/easybuild/modulefiles/daint-mc\\n\")\n",
    "    f.write(\"module load /apps/daint/system/modulefiles/sarus/1.1.0\\n\\n\")\n",
    "    f.write(\"mkdir -p \" + output_dir + \" \" + mrtrix_output + \" \" + fmriprep_output + \" \" + fmriprep_workdir + \" \" + tvb_output + \" \" + tvb_workdir + \"\\n\\n\")    \n",
    "    f.write(\"srun sarus pull thevirtualbrain/tvb-pipeline-sc:1.0\\n\\n\")\n",
    "    \n",
    "    f.write(\"srun sarus run \" + \n",
    "            \"--mount=type=bind,source=$HOME,target=$HOME \" + \n",
    "            \"--mount=type=bind,source=\" + input_dir + \",target=/BIDS_dataset \" +\n",
    "            \"--mount=type=bind,source=\" + mrtrix_output + \",target=/mrtrix3_out \" +\n",
    "            \"--entrypoint \" + # necessary for sarus, to overwrite default entrypoint\n",
    "            \"thevirtualbrain/tvb-pipeline-sc:1.0 python -c \" + \n",
    "                \"\\\"from mrtrix3 import app; app.cleanup=False; import sys; sys.argv=\" + \n",
    "                    \"'/mrtrix3_connectome.py /BIDS_dataset /mrtrix3_out participant1 \" + \n",
    "                    \"--participant_label \" + participant_label + \" -skip \"\n",
    "                    \" --output_verbosity 2 --template_reg ants --n_cpus $SLURM_CPUS_PER_TASK --debug'\" \n",
    "                    \".split(); execfile('/mrtrix3_connectome.py')\\\"\")\n",
    "    \n",
    "    \n",
    "#        f.write(\"srun sarus run \" + \n",
    "#            \"--mount=type=bind,source=$HOME,target=$HOME \" + \n",
    "#            \"--mount=type=bind,source=\" + input_dir + \",target=/BIDS_dataset \" +\n",
    "#            \"--mount=type=bind,source=\" + mrtrix_output + \",target=/mrtrix3_out \" +\n",
    "#            \"--entrypoint \" + # necessary for sarus, to overwrite default entrypoint\n",
    "#            \"thevirtualbrain/tvb-pipeline-sc:1.0 python -c \" + \n",
    "#                \"\\\"from mrtrix3 import app; app.cleanup=False; import sys; sys.argv=\" + \n",
    "#                    \"'/mrtrix3_connectome.py /BIDS_dataset /mrtrix3_out participant1 \" + \n",
    "#                    \"--participant_label \" + participant_label + \" --parcellation \" + parcellation +\n",
    "#                    \" --output_verbosity 2 --template_reg ants --n_cpus $SLURM_CPUS_PER_TASK --debug'\" \n",
    "#                    \".split(); execfile('/mrtrix3_connectome.py')\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the job script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash -l\n",
      "#SBATCH --time=23:59:00\n",
      "#SBATCH --output=slurm-job_script.out\n",
      "#SBATCH --nodes=1\n",
      "#SBATCH --ntasks-per-core=1\n",
      "#SBATCH --ntasks-per-node=1\n",
      "#SBATCH --cpus-per-task=36\n",
      "#SBATCH --partition=normal\n",
      "#SBATCH --constraint=mc\n",
      "#SBATCH --hint=nomultithread\n",
      "export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\n",
      "module load /apps/daint/UES/easybuild/modulefiles/daint-mc\n",
      "module load /apps/daint/system/modulefiles/sarus/1.1.0\n",
      "\n",
      "mkdir -p /scratch/snx3000/unicore/FILESPACE/2dc283ff-6984-44f3-95b8-2ab7bd7bb44f/tvb_converter_workdir /scratch/snx3000/unicore/FILESPACE/2dc283ff-6984-44f3-95b8-2ab7bd7bb44f/tvb_converter_workdir/mrtrix_output /scratch/snx3000/unicore/FILESPACE/2dc283ff-6984-44f3-95b8-2ab7bd7bb44f/tvb_converter_workdir/fmriprep_output /scratch/snx3000/unicore/FILESPACE/2dc283ff-6984-44f3-95b8-2ab7bd7bb44f/tvb_converter_workdir/fmriprep_output/tmp /scratch/snx3000/unicore/FILESPACE/2dc283ff-6984-44f3-95b8-2ab7bd7bb44f/tvb_converter_workdir/TVB_output /scratch/snx3000/unicore/FILESPACE/2dc283ff-6984-44f3-95b8-2ab7bd7bb44f/tvb_converter_workdir/TVB_output/tmp\n",
      "\n",
      "srun sarus pull thevirtualbrain/tvb-pipeline-sc:1.0\n",
      "\n",
      "srun sarus run --mount=type=bind,source=$HOME,target=$HOME --mount=type=bind,source=/scratch/snx3000/unicore/FILESPACE/2dc283ff-6984-44f3-95b8-2ab7bd7bb44f/dataset,target=/BIDS_dataset --mount=type=bind,source=/scratch/snx3000/unicore/FILESPACE/2dc283ff-6984-44f3-95b8-2ab7bd7bb44f/tvb_converter_workdir/mrtrix_output,target=/mrtrix3_out --entrypoint thevirtualbrain/tvb-pipeline-sc:1.0 python -c \"from mrtrix3 import app; app.cleanup=False; import sys; sys.argv='/mrtrix3_connectome.py /BIDS_dataset /mrtrix3_out participant1 --participant_label CON03 -skip  --output_verbosity 2 --template_reg ants --n_cpus $SLURM_CPUS_PER_TASK --debug'.split(); execfile('/mrtrix3_connectome.py')\"\n"
     ]
    }
   ],
   "source": [
    "with open(job_script_path, 'r') as fin:\n",
    "    print(fin.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking good!\n",
    "\n",
    "## 6. Upload SLURM script to supercomputer\n",
    "\n",
    "Here we use the same upload function as used previously for the brain model data followed by a quick check whether the file arrived."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'UNICORE_SCRIPT_EXIT_CODE': PathFile: UNICORE_SCRIPT_EXIT_CODE,\n",
       " 'UNICORE_SCRIPT_PID': PathFile: UNICORE_SCRIPT_PID,\n",
       " 'stderr': PathFile: stderr,\n",
       " 'job_script': PathFile: job_script,\n",
       " 'dataset/': PathDir: dataset/,\n",
       " 'stdout': PathFile: stdout,\n",
       " '__MACOSX/': PathDir: __MACOSX/}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wd_handle.upload(input_name=job_script_path, destination = job_script)\n",
    "wd_handle.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a file with the name stored in the variable `job_script` exists, the upload worked.\n",
    "\n",
    "## 7. Launching the simulation on the supercomputer\n",
    "\n",
    "Now we will run the first out of the three BIDS Apps: mrtrix3_connectome.  \n",
    "We do this by executing the the SLURM command `sbatch <job_script>`, which will evaluate our batch file and generate a job out of it that is added to the queue.\n",
    "After we executed the job, we extract the working directory of this job (which will again be a new directory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/scratch/snx3000/unicore/FILESPACE/0088f7c3-4077-47ca-92af-ee80eaeca747/'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrtrix_job = site_client.execute(\"sbatch \" + base_folder + job_script)\n",
    "wd_mrtrix_res = mrtrix_job.working_dir.properties['mountPoint'].encode('ascii')\n",
    "wd_mrtrix_res = wd_mrtrix_res.decode('utf-8')\n",
    "wd_mrtrix_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have two important folder handles and associated folder paths now. The following table gives an overview over handles, associated paths, and their contents.\n",
    "\n",
    "\n",
    "Handle | Path variable | Description\n",
    ":---: | :---: | :---:\n",
    "`wd_handle` | `base_folder` | BIDS input, pipeline output\n",
    "`mrtrix_job` | `wd_mrtrix_res` | SLURM job meta output\n",
    "\n",
    "\n",
    "You can use the handles and path variables to access files and folders contained in the respective folders, which may be helpful, e.g. for debugging or to download other results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Checking the structural connectome\n",
    "\n",
    "The MRtrix pipeline takes some time, depending on parameters like: number of generated tracks, parcellation, resolution of imaging data, number of parallel threads, etc.\n",
    "\n",
    "With our test dataset and configuration it took around 5 hours.  \n",
    "\n",
    "Let's check whether it finished successfully by inspeting the last lines of the SLURM meta output, which gives us information related to the job execution.\n",
    "\n",
    "Note that after the job was submitted to the queue it usually takes a while until the job starts. Until then you will receive a \"404 Client Error: Not found for URL\" error message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'> extracting     : \"/scratch/snx3000/bp000225/.sarus/cache/sha256:1898ce6247527866b535f3e91cd5e964dcf012e4dfb9569a9688a52bf543b7f1.tar\"\\n',\n",
       " b'> extracting     : \"/scratch/snx3000/bp000225/.sarus/cache/sha256:e88b3011503fa6b7d195696c2428a2456bb03c4afb560a986e7d05b2e7c820ab.tar\"\\n',\n",
       " b'> extracting     : \"/scratch/snx3000/bp000225/.sarus/cache/sha256:96c7fcaee4deef3f52e874f4b434488d7c0553d49a807ea5822bb3f84b7e613c.tar\"\\n',\n",
       " b'> extracting     : \"/scratch/snx3000/bp000225/.sarus/cache/sha256:60644d31c59d5a22624716dbe68ccf89dc38b52312a153d67ff07fa0988b6159.tar\"\\n',\n",
       " b'> extracting     : \"/scratch/snx3000/bp000225/.sarus/cache/sha256:9d3ce4e5876875d880dcb19295e404876a05723fbf39882c8676399a6176e922.tar\"\\n',\n",
       " b'> extracting     : \"/scratch/snx3000/bp000225/.sarus/cache/sha256:231cad235466b6d86dbe6946b10afcbca7504e2d93e1fda2002fc478916f7e16.tar\"\\n',\n",
       " b'> extracting     : \"/scratch/snx3000/bp000225/.sarus/cache/sha256:b2a3cf8961c46db36e486825f2ed2a15a685d18c2ef44b21ed3126669019e45f.tar\"\\n',\n",
       " b'> extracting     : \"/scratch/snx3000/bp000225/.sarus/cache/sha256:7723eee89e4191ec1649cdbecb4819e25755077cfc0c895589d74066dc3c773a.tar\"\\n',\n",
       " b'> extracting     : \"/scratch/snx3000/bp000225/.sarus/cache/sha256:5a2055d7acc7f4791532a8704caba8068df7fc68516ecedde603e0cc02383a7a.tar\"\\n',\n",
       " b'> extracting     : \"/scratch/snx3000/bp000225/.sarus/cache/sha256:d23af71eabae137dbb8c66219daf5f0a1d24d07a6b2343b719ab3478b4e07883.tar\"\\n',\n",
       " b'> extracting     : \"/scratch/snx3000/bp000225/.sarus/cache/sha256:d345549c067bb2af2cb6500b3afb9193f48c08b661966a0805f3dcad8e6d9a04.tar\"\\n',\n",
       " b'> extracting     : \"/scratch/snx3000/bp000225/.sarus/cache/sha256:795825ef36e39fa47908674fa0fe909245ba9a2977bcfbff71219ec187060bae.tar\"\\n',\n",
       " b'> extracting     : \"/scratch/snx3000/bp000225/.sarus/cache/sha256:f630d6f80660754598332182dbbbc8423b2f457cb8b068326aef003c922df459.tar\"\\n',\n",
       " b'> extracting     : \"/scratch/snx3000/bp000225/.sarus/cache/sha256:7a56c7a9b2cc5165b5b2a35e9023a29401beb5f9aa2b8b885dabc5c62fc6978c.tar\"\\n',\n",
       " b'> extracting     : \"/scratch/snx3000/bp000225/.sarus/cache/sha256:e0e369c6be1df9068287366174848e0551bf267a52f5ae708798c361f00d1468.tar\"\\n',\n",
       " b'> extracting     : \"/scratch/snx3000/bp000225/.sarus/cache/sha256:dddfcdd03321bfa511bb608e9002bcaa7f0f3c770eb9af8293d5b245f2966766.tar\"\\n',\n",
       " b'> extracting     : \"/scratch/snx3000/bp000225/.sarus/cache/sha256:f6d44ca3ea2036e5cb8863629e189b95edbeb94ad3a42453df8158ee4f3df987.tar\"\\n',\n",
       " b'> extracting     : \"/scratch/snx3000/bp000225/.sarus/cache/sha256:f31b8e249675229aeb0689962bd2d7f8b6bd19c774ad288344ebdf3a648bb3c5.tar\"\\n',\n",
       " b'> extracting     : \"/scratch/snx3000/bp000225/.sarus/cache/sha256:70c066eb08edc96e9e00d5796415681c29c39ba7eff2acecd142aed232ee95db.tar\"\\n',\n",
       " b'> extracting     : \"/scratch/snx3000/bp000225/.sarus/cache/sha256:f574821f37811d52af490af025c978b7e722b69d6b95f965b6318ddad8d86d6f.tar\"\\n',\n",
       " b'> extracting     : \"/scratch/snx3000/bp000225/.sarus/cache/sha256:fe44708e61d1a671825aaae4464fb981a6e1173a7f99e21c148fcd5ec922951c.tar\"\\n',\n",
       " b'> extracting     : \"/scratch/snx3000/bp000225/.sarus/cache/sha256:fe84271073cd961dda7c17f4f430515880527d9a8164318586555abe0a2cb7dd.tar\"\\n',\n",
       " b'> extracting     : \"/scratch/snx3000/bp000225/.sarus/cache/sha256:94f404830d560e58930b8f3e3e522bc407caa62c6ade6b92f9b8e24a88c55e4f.tar\"\\n',\n",
       " b'> extracting     : \"/scratch/snx3000/bp000225/.sarus/cache/sha256:36cd99d122e4c3c2b305360d75063e82f56c021dfa61aa4636ef84ed02789036.tar\"\\n',\n",
       " b'> extracting     : \"/scratch/snx3000/bp000225/.sarus/cache/sha256:fa5600df7abb8c254e5e8488eb0ca4f8d8395bf6e2322955d6ebaf94153b5efd.tar\"\\n',\n",
       " b'> extracting     : \"/scratch/snx3000/bp000225/.sarus/cache/sha256:2dc041b522bddd454fe3bf3ede7f2bf0957eca3180a34aef3d2c73b3358d4032.tar\"\\n',\n",
       " b'> extracting     : \"/scratch/snx3000/bp000225/.sarus/cache/sha256:1214f24cb017825407e20a568e19a84306a3c1b3b8f2f436cd54597117418d7a.tar\"\\n',\n",
       " b'> extracting     : \"/scratch/snx3000/bp000225/.sarus/cache/sha256:ed5cc65d05ea1792b50ff7ab71b3e0fac3d6846187afc452afd4eac4ffd7112d.tar\"\\n',\n",
       " b'> extracting     : \"/scratch/snx3000/bp000225/.sarus/cache/sha256:faf17815777dcf0fffe7131eaec372f3f76964fa4a2e948365a181814a3f8cb5.tar\"\\n',\n",
       " b'> extracting     : \"/scratch/snx3000/bp000225/.sarus/cache/sha256:fc1947e6f97dd13137e286c9c5e92376ffc027b01017a50fcb26e7949ccb9e5a.tar\"\\n',\n",
       " b'> extracting     : \"/scratch/snx3000/bp000225/.sarus/cache/sha256:f7e5dee872a5b391fae166f8b6e6f603cb056744c4393254bf8980761d29e3c8.tar\"\\n',\n",
       " b'> extracting     : \"/scratch/snx3000/bp000225/.sarus/cache/sha256:a99b92f4901ae8f23fd7e75c357af108931efc9f144e5ce15eb17f528c0dad91.tar\"\\n',\n",
       " b'> extracting     : \"/scratch/snx3000/bp000225/.sarus/cache/sha256:f750c4b6d29ef842e761aff8f5b24e62a4be10f190a8c3baf2b016e660545522.tar\"\\n',\n",
       " b'> extracting     : \"/scratch/snx3000/bp000225/.sarus/cache/sha256:1f3d4d08cdf0bbf413f6cf1c800555234ea7dc2e77a09df1c1ef6ab73845b1c3.tar\"\\n',\n",
       " b'> extracting     : \"/scratch/snx3000/bp000225/.sarus/cache/sha256:eeaebc7962ba0ef66578c1545f55a5f906e94ef402a2c40cae28dce7e326da58.tar\"\\n',\n",
       " b'> extracting     : \"/scratch/snx3000/bp000225/.sarus/cache/sha256:0b7ab956d57c2d5daad87bf5c94063fa23f24e0f7e092f5e185f1a70b765fa8f.tar\"\\n',\n",
       " b'> extracting     : \"/scratch/snx3000/bp000225/.sarus/cache/sha256:de078455bdd5d84ea317677360cc8a8cf43b80184facd4bb3e2b16b2a13cccf7.tar\"\\n',\n",
       " b'> extracting     : \"/scratch/snx3000/bp000225/.sarus/cache/sha256:bf5bbcb69e727f9f0f9f4f1b15dd3bd88c78f5e2a01422d1c28f8a0fa4957be3.tar\"\\n',\n",
       " b'> extracting     : \"/scratch/snx3000/bp000225/.sarus/cache/sha256:d1deab60111cc8f342f474d29ee8ba59c1183f9a8706462beab695777c9af44a.tar\"\\n',\n",
       " b'> make squashfs image: \"/scratch/snx3000/bp000225/.sarus/images/index.docker.io/thevirtualbrain/tvb-pipeline-sc/1.0.squashfs\"\\n']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slurm_output = mrtrix_job.working_dir.stat(\"slurm-\" + job_script + \".out\").raw().readlines()\n",
    "slurm_output[-40:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the job finished successfully, the last few lines of the output should contain a batch job summary result, looking like this:\n",
    "\n",
    "'mrtrix3_connectome.py: Contents of temporary directory kept; location: /mrtrix3_out/mrtrix3_connectome.py-tmp-2BNQRE/\\n',  \n",
    " ' \\n',  \n",
    " 'Batch Job Summary Report for Job \"job_script\" (20156225) on daint\\n',  \n",
    " '-----------------------------------------------------------------------------------------------------\\n',  \n",
    " '             Submit            Eligible               Start                 End    Elapsed  Timelimit \\n',  \n",
    " '------------------- ------------------- ------------------- ------------------- ---------- ---------- \\n',  \n",
    " '2020-02-11T09:13:50 2020-02-11T09:13:50 2020-02-11T09:13:50 2020-02-11T13:45:58   04:32:08   23:59:00 \\n',  \n",
    " '-----------------------------------------------------------------------------------------------------\\n',  \n",
    " 'Username    Account     Partition   NNodes   Energy\\n',  \n",
    " '----------  ----------  ----------  ------  --------------\\n',  \n",
    " 'bp000225    ich012      normal           1        0 joules\\n',  \n",
    " ' \\n',  \n",
    " 'This job did not utilize any GPUs\\n',  \n",
    " ' \\n',  \n",
    " '----------------------------------------------------------\\n',  \n",
    " 'Scratch File System        Files       Quota\\n',  \n",
    " '--------------------  ----------  ----------\\n',  \n",
    " '/scratch/snx3000            1207     1000000\\n',  \n",
    " ' \\n']  \n",
    " \n",
    " \n",
    "\n",
    "\n",
    "\n",
    "After we made sure that the job finished we now fetch a handle to the result folder where the final connectome matrix is stored. The file path format, relative to the base directory of wd_handle of the job working directory, is\n",
    "\n",
    "tvb_converter_workdir/mrtrix_output/sub-<participant_label>/connectome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "connectome_folder = \"tvb_converter_workdir/mrtrix_output/sub-\" + participant_label + \"/connectome\"\n",
    "SC_folder = wd_handle.stat(connectome_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Re-use FreeSurfer's `recon-all` output\n",
    "\n",
    "If you used any of the parcellations {\"desikan\", \"destrieux\", \"hcpmmp1\"} you can re-use recon-all output during the `fmriprep` step. Otherwise, `fmriprep` will run `recon-all` again, which increases computation time.  \n",
    "\n",
    "Using the specified demo filenames, FreeSurfer output is stored in the folder \n",
    "\n",
    "```\n",
    "<base_folder>/tvb_converter_workdir/mrtrix_output/mrtrix3_connectome.py-tmp-<ID string>/freesurfer\n",
    "```\n",
    "\n",
    "Note how the MRtrix App generates a folder \n",
    "\n",
    "```\n",
    "mrtrix3_connectome.py-tmp-<ID string>\n",
    "```\n",
    "\n",
    "This folder is created at runtime by the image, so we don't know its name until it's there. So, let's first find out the folder name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tvb_converter_workdir/mrtrix_output/mrtrix3_connectome.py-tmp-ASX3DR/'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrtrix_output_content = wd_handle.contents(path=\"/tvb_converter_workdir/mrtrix_output\") # get contents of folder\n",
    "mrtrix_output_dir = [i for i in mrtrix_output_content['content'] if \"mrtrix3_connectome.py-tmp-\" in i][0].encode('ascii') # get mrtrix3_connectome.py-tmp-<ID string> folder name\n",
    "mrtrix_output_dir = mrtrix_output_dir.decode('utf-8')\n",
    "mrtrix_output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the MRtrix results folder name to build the FreeSurfer folder path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tvb_converter_workdir/mrtrix_output/mrtrix3_connectome.py-tmp-ASX3DR/freesurfer'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freesurfer_path = mrtrix_output_dir + \"freesurfer\"\n",
    "freesurfer_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good. Now we build the destination path where we copy the freesurfer folder. We copy it to the `fmriprep_output` folder, in a subfolder called  \n",
    "\n",
    "`sub-<participant_label>`.  \n",
    "\n",
    "\n",
    "Due to some technical restrictions of PyUnicore (or my inability to grasp a better solution) we first copy the FreeSurfer folder (in the mrtrix3 App the FreeSurfer subject name is just \"freesurfer\") and then rename it to have the same name as our subject.\n",
    "\n",
    "First, create the target folder and then copy the `freesurfer` folder into the `fmriprep` output folder..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [204]>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freesurfer_target = \"/tvb_converter_workdir/fmriprep_output/freesurfer\"\n",
    "wd_handle.mkdir(freesurfer_target)\n",
    "wd_handle.copy(source=freesurfer_path, target=freesurfer_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [204]>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wd_handle.copy(source=freesurfer_path, target=freesurfer_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, rename the freesurfer folder from subject name `freesurfer` to our current subject name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [204]>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_source = freesurfer_target + \"/freesurfer\"\n",
    "freesurfer_target = freesurfer_target + \"/sub-\" + participant_label\n",
    "wd_handle.rename(source=tmp_source, target=freesurfer_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'action:rename': 'https://brissago.cscs.ch:8080/DAINT-CSCS/rest/core/storages/d385c828-d1fd-4d5f-80e0-4ac310e03802-uspace/actions/rename',\n",
       " 'self': 'https://brissago.cscs.ch:8080/DAINT-CSCS/rest/core/storages/d385c828-d1fd-4d5f-80e0-4ac310e03802-uspace',\n",
       " 'files': 'https://brissago.cscs.ch:8080/DAINT-CSCS/rest/core/storages/d385c828-d1fd-4d5f-80e0-4ac310e03802-uspace/files',\n",
       " 'action:copy': 'https://brissago.cscs.ch:8080/DAINT-CSCS/rest/core/storages/d385c828-d1fd-4d5f-80e0-4ac310e03802-uspace/actions/copy'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wd_handle.path_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`<Response [200]>` of the above operations seems to indicate success! But, to be sure, let's check the folder content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'/tvb_converter_workdir/fmriprep_output/freesurfer/sub-01/tmp/': {'owner': 'bp000225',\n",
       "  'size': 4096,\n",
       "  'permissions': 'rwx------',\n",
       "  'lastAccessed': '2020-05-28T17:00:04+0200',\n",
       "  'isDirectory': True,\n",
       "  'group': 'bp0'},\n",
       " '/tvb_converter_workdir/fmriprep_output/freesurfer/sub-01/trash/': {'owner': 'bp000225',\n",
       "  'size': 4096,\n",
       "  'permissions': 'rwx------',\n",
       "  'lastAccessed': '2020-05-28T17:00:08+0200',\n",
       "  'isDirectory': True,\n",
       "  'group': 'bp0'},\n",
       " '/tvb_converter_workdir/fmriprep_output/freesurfer/sub-01/scripts/': {'owner': 'bp000225',\n",
       "  'size': 4096,\n",
       "  'permissions': 'rwx------',\n",
       "  'lastAccessed': '2020-05-28T16:59:58+0200',\n",
       "  'isDirectory': True,\n",
       "  'group': 'bp0'},\n",
       " '/tvb_converter_workdir/fmriprep_output/freesurfer/sub-01/stats/': {'owner': 'bp000225',\n",
       "  'size': 4096,\n",
       "  'permissions': 'rwx------',\n",
       "  'lastAccessed': '2020-05-28T17:00:08+0200',\n",
       "  'isDirectory': True,\n",
       "  'group': 'bp0'},\n",
       " '/tvb_converter_workdir/fmriprep_output/freesurfer/sub-01/label/': {'owner': 'bp000225',\n",
       "  'size': 4096,\n",
       "  'permissions': 'rwx------',\n",
       "  'lastAccessed': '2020-05-28T17:00:06+0200',\n",
       "  'isDirectory': True,\n",
       "  'group': 'bp0'},\n",
       " '/tvb_converter_workdir/fmriprep_output/freesurfer/sub-01/mri/': {'owner': 'bp000225',\n",
       "  'size': 4096,\n",
       "  'permissions': 'rwx------',\n",
       "  'lastAccessed': '2020-05-28T16:59:59+0200',\n",
       "  'isDirectory': True,\n",
       "  'group': 'bp0'},\n",
       " '/tvb_converter_workdir/fmriprep_output/freesurfer/sub-01/touch/': {'owner': 'bp000225',\n",
       "  'size': 4096,\n",
       "  'permissions': 'rwx------',\n",
       "  'lastAccessed': '2020-05-28T17:00:08+0200',\n",
       "  'isDirectory': True,\n",
       "  'group': 'bp0'},\n",
       " '/tvb_converter_workdir/fmriprep_output/freesurfer/sub-01/surf/': {'owner': 'bp000225',\n",
       "  'size': 4096,\n",
       "  'permissions': 'rwx------',\n",
       "  'lastAccessed': '2020-05-28T17:00:10+0200',\n",
       "  'isDirectory': True,\n",
       "  'group': 'bp0'}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkdir = wd_handle.contents(path=freesurfer_target)\n",
    "checkdir['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks like the typical `recon-all` output folder schema. Nice!  \n",
    "\n",
    "## 10. Upload FreeSurfer license file\n",
    "\n",
    "What we need to do now, in order to finally preprocess the fMRI data, is to upload a FreeSurfer license file. Many will already obtained a FreeSurfer license while downloading/installing FreeSurfer. It is usually located in the FreeSurfer main folder and called  For more information on how to obtain a FreeSurfer license file, see:  \n",
    "\n",
    "https://surfer.nmr.mgh.harvard.edu/fswiki/License\n",
    "\n",
    "Note: the mrtrix3_connectome container already contains a FreeSurfer license, but it is a person-specific license from the container developer, so to be compliant with Usage Terms, please generate and upload your own license file as outlines in the next step.\n",
    "\n",
    "So you generated your license file. Let's upload it. To upload, we perform the same operations as we did to upload the MRI data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload into base_folder\n",
    "wd_handle.upload(input_name = full_fs_path, destination = 'license.txt')\n",
    "wd_handle.listdir()\n",
    "# create file path\n",
    "fs_license = base_folder + \"license.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Run BIDS App `fmriprep` for fMRI preprocessing\n",
    "\n",
    "Ok, everything is in place, let's create a batch file for `fmriprep`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADJUSTABLE PARAMETERS\n",
    "################################################\n",
    "job_script_fmriprep = \"job_script_fmriprep\" # name of the job script file\n",
    "wall_time = \"23:59:00\"\n",
    "aroma_melodic_dimensionality = \"-120\"\n",
    "################################################\n",
    "\n",
    "with open(job_script_fmriprep, \"w\") as f:\n",
    "    f.write(\"#!/bin/bash -l\\n\")  \n",
    "    f.write(\"#SBATCH --time=\" + wall_time + \"\\n\")\n",
    "    f.write(\"#SBATCH --output=slurm-\" + job_script_fmriprep + \".out\\n\")\n",
    "    f.write(\"#SBATCH --nodes=1\\n\")\n",
    "    f.write(\"#SBATCH --ntasks-per-core=1\\n\")    \n",
    "    f.write(\"#SBATCH --ntasks-per-node=1\\n\")\n",
    "    f.write(\"#SBATCH --cpus-per-task=\" + n_cpus + \"\\n\")\n",
    "    f.write(\"#SBATCH --partition=normal\\n\")\n",
    "    f.write(\"#SBATCH --constraint=mc\\n\")\n",
    "    f.write(\"#SBATCH --hint=nomultithread\\n\") # disable hyperthreading such that all cores become available for multithreading\n",
    "    f.write(\"export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\\n\")\n",
    "    f.write(\"module load /apps/daint/UES/easybuild/modulefiles/daint-mc\\n\")\n",
    "    f.write(\"module load /apps/daint/system/modulefiles/sarus/1.1.0\\n\\n\")\n",
    "    f.write(\"srun sarus pull thevirtualbrain/tvb-pipeline-fmriprep\\n\")\n",
    "    f.write(\"srun sarus run \" + \n",
    "            \"--mount=type=bind,source=$HOME,destination=$HOME \" +\n",
    "            \"--mount=type=bind,source=\" + input_dir + \",destination=/dataset \" +\n",
    "            \"--mount=type=bind,source=\" + fmriprep_output + \",destination=/fmriprep_out/ \" +\n",
    "            \"--mount=type=bind,source=\" + fmriprep_workdir + \",destination=/fmriprep_workdir/ \" +\n",
    "            \"--mount=type=bind,source=\" + fs_license + \",destination=/license.txt \" +\n",
    "            \"thevirtualbrain/tvb-pipeline-fmriprep \" +\n",
    "            \"/dataset /fmriprep_out/ participant \" +\n",
    "            \"--use-aroma --bold2t1w-dof 6 --nthreads $SLURM_CPUS_PER_TASK \" +\n",
    "            \"--omp-nthreads $SLURM_CPUS_PER_TASK \" +\n",
    "            \"--output-spaces T1w MNI152NLin6Asym:res-2 fsaverage5 \" +\n",
    "            \"--participant_label \" + participant_label + \" \" +\n",
    "            \"--fs-license-file /license.txt \" +\n",
    "            \"--aroma-melodic-dimensionality \" + aroma_melodic_dimensionality + \" \" +\n",
    "            \"-w /fmriprep_workdir\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check whether it looks good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash -l\n",
      "#SBATCH --time=23:59:00\n",
      "#SBATCH --output=slurm-job_script_fmriprep.out\n",
      "#SBATCH --nodes=1\n",
      "#SBATCH --ntasks-per-core=1\n",
      "#SBATCH --ntasks-per-node=1\n",
      "#SBATCH --cpus-per-task=36\n",
      "#SBATCH --partition=normal\n",
      "#SBATCH --constraint=mc\n",
      "#SBATCH --hint=nomultithread\n",
      "export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\n",
      "module load /apps/daint/UES/easybuild/modulefiles/daint-mc\n",
      "module load /apps/daint/system/modulefiles/sarus/1.1.0\n",
      "\n",
      "srun sarus pull poldracklab/fmriprep\n",
      "srun sarus run --mount=type=bind,source=$HOME,destination=$HOME --mount=type=bind,source=/scratch/snx3000/unicore/FILESPACE/d385c828-d1fd-4d5f-80e0-4ac310e03802/dataset,destination=/dataset --mount=type=bind,source=/scratch/snx3000/unicore/FILESPACE/d385c828-d1fd-4d5f-80e0-4ac310e03802/tvb_converter_workdir/fmriprep_output,destination=/fmriprep_out/ --mount=type=bind,source=/scratch/snx3000/unicore/FILESPACE/d385c828-d1fd-4d5f-80e0-4ac310e03802/tvb_converter_workdir/fmriprep_output/tmp,destination=/fmriprep_workdir/ --mount=type=bind,source=/scratch/snx3000/unicore/FILESPACE/d385c828-d1fd-4d5f-80e0-4ac310e03802/license.txt,destination=/license.txt poldracklab/fmriprep /dataset /fmriprep_out/ participant --use-aroma --bold2t1w-dof 6 --nthreads $SLURM_CPUS_PER_TASK --omp-nthreads $SLURM_CPUS_PER_TASK --output-spaces T1w MNI152NLin6Asym:res-2 fsaverage5 --participant_label 01 --fs-license-file /license.txt --aroma-melodic-dimensionality -120 -w /fmriprep_workdir\n"
     ]
    }
   ],
   "source": [
    "!cat job_script_fmriprep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'UNICORE_SCRIPT_EXIT_CODE': PathFile: UNICORE_SCRIPT_EXIT_CODE,\n",
       " 'UNICORE_SCRIPT_PID': PathFile: UNICORE_SCRIPT_PID,\n",
       " 'stderr': PathFile: stderr,\n",
       " 'license.txt': PathFile: license.txt,\n",
       " 'job_script': PathFile: job_script,\n",
       " 'dataset/': PathDir: dataset/,\n",
       " 'job_script_fmriprep': PathFile: job_script_fmriprep,\n",
       " 'stdout': PathFile: stdout,\n",
       " '__MACOSX/': PathDir: __MACOSX/,\n",
       " 'tvb_converter_workdir/': PathDir: tvb_converter_workdir/}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wd_handle.upload(input_name=job_script_fmriprep, destination = job_script_fmriprep)\n",
    "wd_handle.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file is there, upload worked. Time to submit the batch job to the queue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'/scratch/snx3000/unicore/FILESPACE/cb8f84ef-b2c7-4a01-a2ad-8735a713ec99/'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmriprep_job = site_client.execute(\"sbatch \" + base_folder + job_script_fmriprep)\n",
    "wd_fmriprep = fmriprep_job.working_dir.properties['mountPoint'].encode('ascii')\n",
    "wd_fmriprep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me just update the table to give us a better overview over handles/folders.\n",
    "\n",
    "Handle | Path variable | Description\n",
    ":---: | :---: | :---:\n",
    "`wd_handle` | `base_folder` | BIDS input, pipeline output\n",
    "`mrtrix_job` | `wd_mrtrix_res` | SLURM job meta output MRtrix3\n",
    "`fmriprep_job` | `wd_fmriprep` | SLURM job meta output fmriprep\n",
    "\n",
    "\n",
    "Now let's check whether fmriprep is (still) running or finished.\n",
    "\n",
    "# Please note: There seems to be a strange bug in Sarus: sometimes the command   \n",
    "\n",
    "# `srun sarus pull thevirtualbrain/tvb-pipeline-fmriprep`   \n",
    "\n",
    "# works and sometimes not. It's often easier to just log into your supercomputing account via SSH and run the pull command in the shell. Once the container with the \"latest\" tag was pulled it doesn't need to be re-pulled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'# image            : index.docker.io/poldracklab/fmriprep:latest\\n',\n",
       " b'# cache directory  : \"/scratch/snx3000/bp000225/.sarus/cache\"\\n',\n",
       " b'# temp directory   : \"/tmp\"\\n',\n",
       " b'# images directory : \"/scratch/snx3000/bp000225/.sarus/images\"\\n',\n",
       " b'> save image layers ...\\n',\n",
       " b'> found in cache : sha256:b859591002b6d9f24b9df7427b56290084866a327b33c92d5a43aa13a6327b49\\n',\n",
       " b'> found in cache : sha256:8de46f1b04d0fae16915dcc8896d1af6354601a8602be7435f6c5a3446b6830c\\n',\n",
       " b'> found in cache : sha256:b21d5cc69c8fb02df67725712134ad6d7123d9cea585dc524c5ba03734435000\\n',\n",
       " b'> found in cache : sha256:a0f74950c55bb6b072add5997c5b4693182d82bcbd58f000cb1bf107419ce0e4\\n',\n",
       " b'> found in cache : sha256:a82cc7ba20ad040d1ae6b0e1e53bee7fc72d53707b992fb6f8f1e5a7aac35a2a\\n',\n",
       " b'> found in cache : sha256:42da0942cc0e3de8b86ba649f506f3a0c87533451756c51d7b8bf181ba94a2eb\\n',\n",
       " b'> found in cache : sha256:7abb3d638ec4853b85fef7192689023a3d394b78198bd8e7894e3857106687a7\\n',\n",
       " b'> found in cache : sha256:19197c55075519928dd2ff059745665a2c9b72f4e8af6f7a1ce662e696d339bd\\n',\n",
       " b'> found in cache : sha256:ac0b78389510ddc5f05baf66cce3660bd59dcc6c40f868b039805e42f61755fb\\n',\n",
       " b'> found in cache : sha256:f7867020ff2ce265c91fbaa2ed9caff9f4ec36412796c9f8ed19829f938aca29\\n',\n",
       " b'> found in cache : sha256:611fe4f705a58c51fd740b790122f3f5900386b72f3a4c5bed2f8bd6b5c28a19\\n',\n",
       " b'> found in cache : sha256:759836a2d54d95423af6e30fd1947ce82ea3a4ca8214c41cbb0a8d08529a6d11\\n',\n",
       " b'> found in cache : sha256:557abc17033b8821a4f11bb7a28128494c6d5fc2a98a1e08150eba991ef0a6f4\\n',\n",
       " b'> found in cache : sha256:4a5a32a8b67f8f0d427ade65f52b063bd6e7ba448e5bdfda3e3da19f68369ff5\\n',\n",
       " b'> found in cache : sha256:0a01a72a686c389637334de1e2d0012da298960366f6d8f358b8e10dc3b5e330\\n',\n",
       " b'> found in cache : sha256:9e499ad45d567b53c6c6b5a72f4b2d7b37da99751cbc4368d244f517c8951e4d> found in cache : sha256:14f5757104e98f3e93ab8930b4afd4eb21146c829f97cb5f942f31f67419dfc1\\n',\n",
       " b'\\n',\n",
       " b'> found in cache : sha256:b5bf898e214a893171c1e1ab287fc7f1d3573e414869f21f06e3468fea43add3\\n',\n",
       " b'> found in cache : sha256:716d454e56b61d1343a01f3b1829574333e2e3df20e77d1958d7b0b939ea1b61\\n',\n",
       " b'> found in cache : sha256:cc899a5544da1a6cfb970d2484d32c063f8df26a430d92f39c98e72261e226f2\\n',\n",
       " b'> pulling        : sha256:ddbeaf87813e4b4c859d007ac0ae3e0726414d34317b519a833d507031329c5d\\n',\n",
       " b'> pulling        : sha256:828bf7558000792b20fcf2f9e48100dd9e4f8212991fa6d467e4eb3770c11594\\n',\n",
       " b'> pulling        : sha256:4c5c5e57a6813651b2626f49f6c741ddc7ee2f11b11ef8fa47436c9ab7343570\\n',\n",
       " b'> pulling        : sha256:8797e74e7dc883c3d77bd0e02e6199c0828c8af3000f67a9525dccb4aad7a33a\\n',\n",
       " b'> pulling        : sha256:0461199c2371eb477ad38694530037303b76afa83f5e38c8de1e1adaabb7842d\\n',\n",
       " b'> pulling        : sha256:3e3a9315ec881cb8b9c913cc979655e9262aee0752aba0b8b52f264e034df4a9\\n',\n",
       " b'> pulling        : sha256:e77819e8db0ad3b3be19b7c81b043abbfdf066c666553d978755b9af1e54c847\\n',\n",
       " b'> completed      : sha256:4c5c5e57a6813651b2626f49f6c741ddc7ee2f11b11ef8fa47436c9ab7343570\\n',\n",
       " b'> completed      : sha256:8797e74e7dc883c3d77bd0e02e6199c0828c8af3000f67a9525dccb4aad7a33a\\n',\n",
       " b'> completed      : sha256:ddbeaf87813e4b4c859d007ac0ae3e0726414d34317b519a833d507031329c5d\\n',\n",
       " b'> completed      : sha256:0461199c2371eb477ad38694530037303b76afa83f5e38c8de1e1adaabb7842d\\n',\n",
       " b'> completed      : sha256:e77819e8db0ad3b3be19b7c81b043abbfdf066c666553d978755b9af1e54c847\\n',\n",
       " b'> completed      : sha256:828bf7558000792b20fcf2f9e48100dd9e4f8212991fa6d467e4eb3770c11594\\n',\n",
       " b'> completed      : sha256:3e3a9315ec881cb8b9c913cc979655e9262aee0752aba0b8b52f264e034df4a9\\n',\n",
       " b'> expanding image layers ...\\n',\n",
       " b'> extracting     : \"/scratch/snx3000/bp000225/.sarus/cache/sha256:0a01a72a686c389637334de1e2d0012da298960366f6d8f358b8e10dc3b5e330.tar\"\\n',\n",
       " b'> extracting     : \"/scratch/snx3000/bp000225/.sarus/cache/sha256:cc899a5544da1a6cfb970d2484d32c063f8df26a430d92f39c98e72261e226f2.tar\"\\n',\n",
       " b'> extracting     : \"/scratch/snx3000/bp000225/.sarus/cache/sha256:19197c55075519928dd2ff059745665a2c9b72f4e8af6f7a1ce662e696d339bd.tar\"\\n',\n",
       " b'> extracting     : \"/scratch/snx3000/bp000225/.sarus/cache/sha256:716d454e56b61d1343a01f3b1829574333e2e3df20e77d1958d7b0b939ea1b61.tar\"\\n',\n",
       " b'> extracting     : \"/scratch/snx3000/bp000225/.sarus/cache/sha256:b5bf898e214a893171c1e1ab287fc7f1d3573e414869f21f06e3468fea43add3.tar\"\\n',\n",
       " b'> extracting     : \"/scratch/snx3000/bp000225/.sarus/cache/sha256:42da0942cc0e3de8b86ba649f506f3a0c87533451756c51d7b8bf181ba94a2eb.tar\"\\n',\n",
       " b'> extracting     : \"/scratch/snx3000/bp000225/.sarus/cache/sha256:14f5757104e98f3e93ab8930b4afd4eb21146c829f97cb5f942f31f67419dfc1.tar\"\\n',\n",
       " b'> extracting     : \"/scratch/snx3000/bp000225/.sarus/cache/sha256:611fe4f705a58c51fd740b790122f3f5900386b72f3a4c5bed2f8bd6b5c28a19.tar\"\\n',\n",
       " b'> extracting     : \"/scratch/snx3000/bp000225/.sarus/cache/sha256:ac0b78389510ddc5f05baf66cce3660bd59dcc6c40f868b039805e42f61755fb.tar\"\\n',\n",
       " b'> extracting     : \"/scratch/snx3000/bp000225/.sarus/cache/sha256:9e499ad45d567b53c6c6b5a72f4b2d7b37da99751cbc4368d244f517c8951e4d.tar\"\\n',\n",
       " b'> extracting     : \"/scratch/snx3000/bp000225/.sarus/cache/sha256:7abb3d638ec4853b85fef7192689023a3d394b78198bd8e7894e3857106687a7.tar\"\\n',\n",
       " b'> extracting     : \"/scratch/snx3000/bp000225/.sarus/cache/sha256:a0f74950c55bb6b072add5997c5b4693182d82bcbd58f000cb1bf107419ce0e4.tar\"\\n',\n",
       " b'> extracting     : \"/scratch/snx3000/bp000225/.sarus/cache/sha256:759836a2d54d95423af6e30fd1947ce82ea3a4ca8214c41cbb0a8d08529a6d11.tar\"\\n',\n",
       " b'> extracting     : \"/scratch/snx3000/bp000225/.sarus/cache/sha256:f7867020ff2ce265c91fbaa2ed9caff9f4ec36412796c9f8ed19829f938aca29.tar\"\\n',\n",
       " b'> extracting     : \"/scratch/snx3000/bp000225/.sarus/cache/sha256:b21d5cc69c8fb02df67725712134ad6d7123d9cea585dc524c5ba03734435000.tar\"\\n',\n",
       " b'> extracting     : \"/scratch/snx3000/bp000225/.sarus/cache/sha256:8de46f1b04d0fae16915dcc8896d1af6354601a8602be7435f6c5a3446b6830c.tar\"\\n',\n",
       " b'> extracting     : \"/scratch/snx3000/bp000225/.sarus/cache/sha256:a82cc7ba20ad040d1ae6b0e1e53bee7fc72d53707b992fb6f8f1e5a7aac35a2a.tar\"\\n',\n",
       " b'> extracting     : \"/scratch/snx3000/bp000225/.sarus/cache/sha256:4a5a32a8b67f8f0d427ade65f52b063bd6e7ba448e5bdfda3e3da19f68369ff5.tar\"\\n',\n",
       " b'> extracting     : \"/scratch/snx3000/bp000225/.sarus/cache/sha256:557abc17033b8821a4f11bb7a28128494c6d5fc2a98a1e08150eba991ef0a6f4.tar\"\\n',\n",
       " b'> extracting     : \"/scratch/snx3000/bp000225/.sarus/cache/sha256:b859591002b6d9f24b9df7427b56290084866a327b33c92d5a43aa13a6327b49.tar\"\\n',\n",
       " b'> extracting     : \"/scratch/snx3000/bp000225/.sarus/cache/sha256:4c5c5e57a6813651b2626f49f6c741ddc7ee2f11b11ef8fa47436c9ab7343570.tar\"\\n',\n",
       " b'> extracting     : \"/scratch/snx3000/bp000225/.sarus/cache/sha256:3e3a9315ec881cb8b9c913cc979655e9262aee0752aba0b8b52f264e034df4a9.tar\"\\n',\n",
       " b'> extracting     : \"/scratch/snx3000/bp000225/.sarus/cache/sha256:0461199c2371eb477ad38694530037303b76afa83f5e38c8de1e1adaabb7842d.tar\"\\n',\n",
       " b'> extracting     : \"/scratch/snx3000/bp000225/.sarus/cache/sha256:e77819e8db0ad3b3be19b7c81b043abbfdf066c666553d978755b9af1e54c847.tar\"\\n',\n",
       " b'> extracting     : \"/scratch/snx3000/bp000225/.sarus/cache/sha256:8797e74e7dc883c3d77bd0e02e6199c0828c8af3000f67a9525dccb4aad7a33a.tar\"\\n',\n",
       " b'> extracting     : \"/scratch/snx3000/bp000225/.sarus/cache/sha256:828bf7558000792b20fcf2f9e48100dd9e4f8212991fa6d467e4eb3770c11594.tar\"\\n',\n",
       " b'> extracting     : \"/scratch/snx3000/bp000225/.sarus/cache/sha256:ddbeaf87813e4b4c859d007ac0ae3e0726414d34317b519a833d507031329c5d.tar\"\\n',\n",
       " b'> make squashfs image: \"/scratch/snx3000/bp000225/.sarus/images/index.docker.io/poldracklab/fmriprep/latest.squashfs\"\\n',\n",
       " b'Detected glibc 2.23 (< 2.26) in the container. Replacing it with glibc 2.26 from the host. Please consider upgrading the container image to a distribution with glibc >= 2.26.\\n',\n",
       " b'bids-validator@1.4.0\\n',\n",
       " b'\\n',\n",
       " b'\\t\\x1b[31m1: [ERR] Files with such naming scheme are not part of BIDS specification. This error is most commonly caused by typos in file names that make them not BIDS compatible. Please consult the specification and make sure your files are named correctly. If this is not a file naming issue (for example when including files not yet covered by the BIDS specification) you should include a \".bidsignore\" file in your dataset (see https://github.com/bids-standard/bids-validator#bidsignore for details). Please note that derived (processed) data should be placed in /derivatives folder and source data (such as DICOMS or behavioural logs in proprietary formats) should be placed in the /sourcedata folder. (code: 1 - NOT_INCLUDED)\\x1b[39m\\n',\n",
       " b'\\t\\t./sub-01/anat/sub-01_ses-00_T1w.nii.gz\\n',\n",
       " b'\\t\\t\\tEvidence: sub-01_ses-00_T1w.nii.gz\\n',\n",
       " b'\\t\\t./sub-01/dwi/sub-01_ses-00_dwi.bval\\n',\n",
       " b'\\t\\t\\tEvidence: sub-01_ses-00_dwi.bval\\n',\n",
       " b'\\t\\t./sub-01/dwi/sub-01_ses-00_dwi.bvec\\n',\n",
       " b'\\t\\t\\tEvidence: sub-01_ses-00_dwi.bvec\\n',\n",
       " b'\\t\\t./sub-01/dwi/sub-01_ses-00_dwi.json\\n',\n",
       " b'\\t\\t\\tEvidence: sub-01_ses-00_dwi.json\\n',\n",
       " b'\\t\\t./sub-01/dwi/sub-01_ses-00_dwi.nii.gz\\n',\n",
       " b'\\t\\t\\tEvidence: sub-01_ses-00_dwi.nii.gz\\n',\n",
       " b'\\t\\t./sub-01/func/sub-01_ses-00_task-ArchiSocial_acq-ap_bold.nii.gz\\n',\n",
       " b'\\t\\t\\tEvidence: sub-01_ses-00_task-ArchiSocial_acq-ap_bold.nii.gz\\n',\n",
       " b'\\t\\t./sub-01/func/sub-01_ses-00_task-ArchiSocial_acq-ap_events.tsv\\n',\n",
       " b'\\t\\t\\tEvidence: sub-01_ses-00_task-ArchiSocial_acq-ap_events.tsv\\n',\n",
       " b'\\t\\t./sub-01/func/sub-01_ses-00_task-ArchiSocial_acq-ap_sbref.nii.gz\\n',\n",
       " b'\\t\\t\\tEvidence: sub-01_ses-00_task-ArchiSocial_acq-ap_sbref.nii.gz\\n',\n",
       " b'\\n',\n",
       " b'\\x1b[36m\\tPlease visit https://neurostars.org/search?q=NOT_INCLUDED for existing conversations about this issue.\\x1b[39m\\n',\n",
       " b'\\n',\n",
       " b'\\t\\x1b[31m2: [ERR] Dataset does not contain any T1w scans. (code: 53 - NO_T1W)\\x1b[39m\\n',\n",
       " b'\\n',\n",
       " b'\\x1b[36m\\tPlease visit https://neurostars.org/search?q=NO_T1W for existing conversations about this issue.\\x1b[39m\\n',\n",
       " b'\\n',\n",
       " b\"\\t\\x1b[31m3: [ERR] Session label in the filename doesn't match with the path of the file. File seems to be saved in incorrect session directory. (code: 65 - SESSION_LABEL_IN_FILENAME_DOESNOT_MATCH_DIRECTORY)\\x1b[39m\\n\",\n",
       " b'\\t\\t./sub-01/anat/sub-01_ses-00_T1w.nii.gz\\n',\n",
       " b'\\t\\t\\tEvidence: File: /sub-01/anat/sub-01_ses-00_T1w.nii.gz is saved in incorrect session directory as per ses-id in filename.\\n',\n",
       " b'\\t\\t./sub-01/dwi/sub-01_ses-00_dwi.bval\\n',\n",
       " b'\\t\\t\\tEvidence: File: /sub-01/dwi/sub-01_ses-00_dwi.bval is saved in incorrect session directory as per ses-id in filename.\\n',\n",
       " b'\\t\\t./sub-01/dwi/sub-01_ses-00_dwi.bvec\\n',\n",
       " b'\\t\\t\\tEvidence: File: /sub-01/dwi/sub-01_ses-00_dwi.bvec is saved in incorrect session directory as per ses-id in filename.\\n',\n",
       " b'\\t\\t./sub-01/dwi/sub-01_ses-00_dwi.json\\n',\n",
       " b'\\t\\t\\tEvidence: File: /sub-01/dwi/sub-01_ses-00_dwi.json is saved in incorrect session directory as per ses-id in filename.\\n',\n",
       " b'\\t\\t./sub-01/dwi/sub-01_ses-00_dwi.nii.gz\\n',\n",
       " b'\\t\\t\\tEvidence: File: /sub-01/dwi/sub-01_ses-00_dwi.nii.gz is saved in incorrect session directory as per ses-id in filename.\\n',\n",
       " b'\\t\\t./sub-01/func/sub-01_ses-00_task-ArchiSocial_acq-ap_bold.nii.gz\\n',\n",
       " b'\\t\\t\\tEvidence: File: /sub-01/func/sub-01_ses-00_task-ArchiSocial_acq-ap_bold.nii.gz is saved in incorrect session directory as per ses-id in filename.\\n',\n",
       " b'\\t\\t./sub-01/func/sub-01_ses-00_task-ArchiSocial_acq-ap_events.tsv\\n',\n",
       " b'\\t\\t\\tEvidence: File: /sub-01/func/sub-01_ses-00_task-ArchiSocial_acq-ap_events.tsv is saved in incorrect session directory as per ses-id in filename.\\n',\n",
       " b'\\t\\t./sub-01/func/sub-01_ses-00_task-ArchiSocial_acq-ap_sbref.nii.gz\\n',\n",
       " b'\\t\\t\\tEvidence: File: /sub-01/func/sub-01_ses-00_task-ArchiSocial_acq-ap_sbref.nii.gz is saved in incorrect session directory as per ses-id in filename.\\n',\n",
       " b'\\n',\n",
       " b'\\x1b[36m\\tPlease visit https://neurostars.org/search?q=SESSION_LABEL_IN_FILENAME_DOESNOT_MATCH_DIRECTORY for existing conversations about this issue.\\x1b[39m\\n',\n",
       " b'\\n',\n",
       " b'\\t\\x1b[31m4: [ERR] No BIDS compatible data found for at least one subject. (code: 67 - NO_VALID_DATA_FOUND_FOR_SUBJECT)\\x1b[39m\\n',\n",
       " b'\\t\\t./sub-01\\n',\n",
       " b'\\n',\n",
       " b'\\x1b[36m\\tPlease visit https://neurostars.org/search?q=NO_VALID_DATA_FOUND_FOR_SUBJECT for existing conversations about this issue.\\x1b[39m\\n',\n",
       " b'\\n',\n",
       " b'\\t\\x1b[31m5: [ERR] EffectiveEchoSpacing should always be smaller than TotalReadoutTime.  (code: 93 - EFFECTIVEECHOSPACING_LARGER_THAN_TOTALREADOUTTIME)\\x1b[39m\\n',\n",
       " b'\\t\\t./dwi.json\\n',\n",
       " b'\\n',\n",
       " b'\\x1b[36m\\tPlease visit https://neurostars.org/search?q=EFFECTIVEECHOSPACING_LARGER_THAN_TOTALREADOUTTIME for existing conversations about this issue.\\x1b[39m\\n',\n",
       " b'\\n',\n",
       " b'\\n',\n",
       " b'        \\x1b[34m\\x1b[4mSummary:\\x1b[24m\\x1b[39m                  \\x1b[34m\\x1b[4mAvailable Tasks:\\x1b[24m\\x1b[39m        \\x1b[34m\\x1b[4mAvailable Modalities:\\x1b[24m\\x1b[39m \\n',\n",
       " b'        17 Files, 456.38MB        archi social                                  \\n',\n",
       " b'        0 - Subjects                                                            \\n',\n",
       " b'        1 - Session                                                             \\n',\n",
       " b'\\n',\n",
       " b'\\n',\n",
       " b'\\x1b[36m\\tIf you have any questions, please post on https://neurostars.org/tags/bids.\\x1b[39m\\n',\n",
       " b'\\n',\n",
       " b'Traceback (most recent call last):\\n',\n",
       " b'  File \"/usr/local/miniconda/bin/fmriprep\", line 10, in <module>\\n',\n",
       " b'    sys.exit(main())\\n',\n",
       " b'  File \"/usr/local/miniconda/lib/python3.7/site-packages/fmriprep/cli/run.py\", line 17, in main\\n',\n",
       " b'    parse_args()\\n',\n",
       " b'  File \"/usr/local/miniconda/lib/python3.7/site-packages/fmriprep/cli/parser.py\", line 659, in parse_args\\n',\n",
       " b'    config.environment.exec_env, opts.bids_dir, opts.participant_label\\n',\n",
       " b'  File \"/usr/local/miniconda/lib/python3.7/site-packages/fmriprep/utils/bids.py\", line 140, in validate_input_dir\\n',\n",
       " b\"    subprocess.check_call(['bids-validator', bids_dir, '-c', temp.name])\\n\",\n",
       " b'  File \"/usr/local/miniconda/lib/python3.7/subprocess.py\", line 341, in check_call\\n',\n",
       " b'    raise CalledProcessError(retcode, cmd)\\n',\n",
       " b\"subprocess.CalledProcessError: Command '['bids-validator', PosixPath('/dataset'), '-c', '/tmp/tmpejcxl9as']' returned non-zero exit status 1.\\n\",\n",
       " b'srun: error: nid01289: task 0: Exited with exit code 1\\n',\n",
       " b'srun: Terminating job step 22827751.1\\n',\n",
       " b' \\n',\n",
       " b'Batch Job Summary Report for Job \"job_script_fmriprep\" (22827751) on daint\\n',\n",
       " b'-----------------------------------------------------------------------------------------------------\\n',\n",
       " b'             Submit            Eligible               Start                 End    Elapsed  Timelimit \\n',\n",
       " b'------------------- ------------------- ------------------- ------------------- ---------- ---------- \\n',\n",
       " b'2020-05-28T17:01:40 2020-05-28T17:01:40 2020-05-28T17:02:18 2020-05-28T17:06:45   00:04:27   23:59:00 \\n',\n",
       " b'-----------------------------------------------------------------------------------------------------\\n',\n",
       " b'Username    Account     Partition   NNodes   Energy\\n',\n",
       " b'----------  ----------  ----------  ------  --------------\\n',\n",
       " b'bp000225    ich012      normal           1   33.10K joules\\n',\n",
       " b' \\n',\n",
       " b'This job did not utilize any GPUs\\n',\n",
       " b' \\n',\n",
       " b'----------------------------------------------------------\\n',\n",
       " b'Scratch File System        Files       Quota\\n',\n",
       " b'--------------------  ----------  ----------\\n',\n",
       " b'/scratch/snx3000            8812     1000000\\n',\n",
       " b' \\n']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slurm_output = fmriprep_job.working_dir.stat(\"slurm-\" + job_script_fmriprep + \".out\").raw().readlines()\n",
    "slurm_output[-40:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You know the drill: unless there is the job summary at the end, the job hasn't finished.\n",
    "\n",
    "## 12. Select `recon-all` output\n",
    "\n",
    "Either of the two Apps, `mrtrix3_connectome` and `fmriprep`, may have run `recon-all` (depending on which parcellation you have chosen). Here, we set the appropriate folder. In this example we used the parcellation `desikan` and `mrtrix3_connectome` generated `recon_all` output which we re-used later in `fmriprep`. So let's configure accordingly. Below are the two alternatives in different cells, choose only one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Option 1: recon_all run by fmriprep\n",
    "recon_all_dir = base_folder + freesurfer_target + \"/freesurfer/\"\n",
    "recon_all_subject_name = \"sub-\" + participant_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Option 2: recon_all run by MRtrix3\n",
    "recon_all_dir = base_folder + freesurfer_path[:-10] # cut-off the last \"freesurfer\"\n",
    "recon_all_subject_name = \"freesurfer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/scratch/snx3000/unicore/FILESPACE/3b57e398-e641-4080-8766-0f94fa3fe913//tvb_converter_workdir/mrtrix_output/mrtrix3_connectome.py-tmp-R3O573/'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recon_all_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Run BIDS App `tvb_converter` to generate TVB input\n",
    "\n",
    "What remains to be done is to create a batch file for the tvb_converter BIDS App, upload it and execute it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADJUSTABLE PARAMETERS\n",
    "################################################\n",
    "wall_time = \"01:00:00\"\n",
    "cpu_per_task = \"36\"\n",
    "job_script_tvb_conv = \"job_script_tvb_conv\" # name of the job script file\n",
    "################################################\n",
    "\n",
    "# FIXED PARAMETERS\n",
    "################################################\n",
    "weights_path = \"/mrtrix3_out/sub-\" + participant_label + \"/connectome/sub-\" + participant_label + \"_parc-\" + parcellation + \"_level-participant_connectome.csv\"\n",
    "tracts_path = \"/mrtrix3_out/sub-\" + participant_label + \"/connectome/sub-\" + participant_label + \"_parc-\"+ parcellation+ \"_meanlength.csv\"\n",
    "################################################\n",
    "\n",
    "\n",
    "with open(job_script_tvb_conv, \"w\") as f:\n",
    "    f.write(\"#!/bin/bash -l\\n\")  \n",
    "    f.write(\"#SBATCH --time=\" + wall_time + \"\\n\")\n",
    "    f.write(\"#SBATCH --output=slurm-\" + job_script_tvb_conv + \".out\\n\")\n",
    "    f.write(\"#SBATCH --nodes=1\\n\")\n",
    "    f.write(\"#SBATCH --ntasks-per-core=1\\n\")    \n",
    "    f.write(\"#SBATCH --ntasks-per-node=1\\n\")\n",
    "    f.write(\"#SBATCH --cpus-per-task=\" + cpu_per_task + \"\\n\")\n",
    "    f.write(\"#SBATCH --partition=normal\\n\")\n",
    "    f.write(\"#SBATCH --constraint=mc\\n\")\n",
    "    f.write(\"#SBATCH --hint=nomultithread\\n\") # disable hyperthreading such that all cores become available for multithreading\n",
    "    f.write(\"export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\\n\")\n",
    "    f.write(\"module load /apps/daint/UES/easybuild/modulefiles/daint-mc\\n\")\n",
    "    f.write(\"module load /apps/daint/system/modulefiles/sarus/1.0.1\\n\")\n",
    "    f.write(\"srun sarus pull thevirtualbrain/tvb-pipeline-converter\\n\")\n",
    "    f.write(\"srun sarus run \" +\n",
    "            \"--mount=type=bind,source=`dirname \" + fs_license + \"`,destination=/freesurfer_license_dir/ \" + \n",
    "            \"--mount=type=bind,source=\" + input_dir + \",destination=/input_dir \" +\n",
    "            \"--mount=type=bind,source=\" + output_dir + \",destination=/output_dir \" +\n",
    "            \"--mount=type=bind,source=\" + mrtrix_output + \",destination=/mrtrix3_out \" +\n",
    "            \"--mount=type=bind,source=\" + fmriprep_output + \",destination=/fmriprep_out \" +\n",
    "            \"--mount=type=bind,source=\" + fmriprep_workdir + \",destination=/fmriprep_workdir \" +\n",
    "            \"--mount=type=bind,source=\" + tvb_output + \",destination=/tvb_out \" +\n",
    "            \"--mount=type=bind,source=\" + tvb_workdir + \",destination=/tvb_workdir \" +\n",
    "            \"--mount=type=bind,source=\" + recon_all_dir + \",destination=/recon_all_dir \" +\n",
    "            \"--entrypoint thevirtualbrain/tvb-pipeline-converter \" +\n",
    "            \"/bin/bash -c \\\"cp /freesurfer_license_dir/license.txt /opt/freesurfer/ && \" +\n",
    "                            \"mkdir -p /recon_all_dir/\" + recon_all_subject_name + \"/bem && \" +\n",
    "                            \"cd /recon_all_dir/\" + recon_all_subject_name + \"/bem && \" +\n",
    "                            \"/tvb_converter_pipeline.sh /input_dir /output_dir /mrtrix3_out /fmriprep_out \" +\n",
    "                            \"/fmriprep_workdir /tvb_out /tvb_workdir /recon_all_dir \" + recon_all_subject_name + \" \" +\n",
    "                            participant_label + \" \" + task_name + \" \" + parcellation + \" \" + weights_path + \n",
    "                            \" \" + tracts_path + \" \" + cpu_per_task + \"\\\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash -l\n",
      "#SBATCH --time=01:00:00\n",
      "#SBATCH --output=slurm-job_script_tvb_conv.out\n",
      "#SBATCH --nodes=1\n",
      "#SBATCH --ntasks-per-core=1\n",
      "#SBATCH --ntasks-per-node=1\n",
      "#SBATCH --cpus-per-task=36\n",
      "#SBATCH --partition=normal\n",
      "#SBATCH --constraint=mc\n",
      "#SBATCH --hint=nomultithread\n",
      "export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\n",
      "module load /apps/daint/UES/easybuild/modulefiles/daint-mc\n",
      "module load /apps/daint/system/modulefiles/sarus/1.0.1\n",
      "srun sarus pull thevirtualbrain/tvb_converter\n",
      "srun sarus run --mount=type=bind,source=`dirname /scratch/snx3000/unicore/FILESPACE/3b57e398-e641-4080-8766-0f94fa3fe913/license.txt`,destination=/freesurfer_license_dir/ --mount=type=bind,source=/scratch/snx3000/unicore/FILESPACE/3b57e398-e641-4080-8766-0f94fa3fe913/BIDS_test,destination=/input_dir --mount=type=bind,source=/scratch/snx3000/unicore/FILESPACE/3b57e398-e641-4080-8766-0f94fa3fe913/tvb_converter_workdir,destination=/output_dir --mount=type=bind,source=/scratch/snx3000/unicore/FILESPACE/3b57e398-e641-4080-8766-0f94fa3fe913/tvb_converter_workdir/mrtrix_output,destination=/mrtrix3_out --mount=type=bind,source=/scratch/snx3000/unicore/FILESPACE/3b57e398-e641-4080-8766-0f94fa3fe913/tvb_converter_workdir/fmriprep_output,destination=/fmriprep_out --mount=type=bind,source=/scratch/snx3000/unicore/FILESPACE/3b57e398-e641-4080-8766-0f94fa3fe913/tvb_converter_workdir/fmriprep_output/tmp,destination=/fmriprep_workdir --mount=type=bind,source=/scratch/snx3000/unicore/FILESPACE/3b57e398-e641-4080-8766-0f94fa3fe913/tvb_converter_workdir/TVB_output,destination=/tvb_out --mount=type=bind,source=/scratch/snx3000/unicore/FILESPACE/3b57e398-e641-4080-8766-0f94fa3fe913/tvb_converter_workdir/TVB_output/tmp,destination=/tvb_workdir --mount=type=bind,source=/scratch/snx3000/unicore/FILESPACE/3b57e398-e641-4080-8766-0f94fa3fe913//tvb_converter_workdir/mrtrix_output/mrtrix3_connectome.py-tmp-R3O573/,destination=/recon_all_dir --entrypoint thevirtualbrain/tvb_converter /bin/bash -c \"cp /freesurfer_license_dir/license.txt /opt/freesurfer/ && mkdir -p /recon_all_dir/freesurfer/bem && cd /recon_all_dir/freesurfer/bem && /tvb_converter_pipeline.sh /input_dir /output_dir /mrtrix3_out /fmriprep_out /fmriprep_workdir /tvb_out /tvb_workdir /recon_all_dir freesurfer QL20120814 rest desikan /mrtrix3_out/sub-QL20120814/connectome/sub-QL20120814_parc-desikan_level-participant_connectome.csv /mrtrix3_out/sub-QL20120814/connectome/sub-QL20120814_parc-desikan_meanlength.csv 36\"\n"
     ]
    }
   ],
   "source": [
    "!cat job_script_tvb_conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'BIDS_test/': PathDir: BIDS_test/,\n",
       " u'UNICORE_SCRIPT_EXIT_CODE': PathFile: UNICORE_SCRIPT_EXIT_CODE,\n",
       " u'UNICORE_SCRIPT_PID': PathFile: UNICORE_SCRIPT_PID,\n",
       " u'__MACOSX/': PathDir: __MACOSX/,\n",
       " u'job_script': PathFile: job_script,\n",
       " u'job_script_fmriprep': PathFile: job_script_fmriprep,\n",
       " u'job_script_tvb_conv': PathFile: job_script_tvb_conv,\n",
       " u'license.txt': PathFile: license.txt,\n",
       " u'stderr': PathFile: stderr,\n",
       " u'stdout': PathFile: stdout,\n",
       " u'tvb_converter_workdir/': PathDir: tvb_converter_workdir/}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wd_handle.upload(input_name=job_script_tvb_conv, destination = job_script_tvb_conv)\n",
    "wd_handle.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file is there, upload worked. Time to submit the batch job to the queue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/scratch/snx3000/unicore/FILESPACE/251c77e3-07b3-4924-a1a7-69d32007583c/'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tvb_conv_job = site_client.execute(\"sbatch \" + base_folder + job_script_tvb_conv)\n",
    "wd_tvb_conv = tvb_conv_job.working_dir.properties['mountPoint'].encode('ascii')\n",
    "wd_tvb_conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's update the table one last time\n",
    "\n",
    "Handle | Path variable | Description\n",
    ":---: | :---: | :---:\n",
    "`wd_handle` | `base_folder` | BIDS input, pipeline output\n",
    "`mrtrix_job` | `wd_mrtrix_res` | SLURM job meta output MRtrix3\n",
    "`fmriprep_job` | `wd_fmriprep` | SLURM job meta output fmriprep\n",
    "`tvb_conv_job` | `wd_tvb_conv` | SLURM job meta output tvb_converter\n",
    "\n",
    "\n",
    "Let's check whether tvb_converter is (still) running or finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['> completed      : sha256:0acb0ab97510317eceec3f64cd6d60a01502e15a9867c739700ae504d3c4fa6a\\n',\n",
       " '> completed      : sha256:d2083d800e5813079efc401a41b5ef17ba3e188759f1dde597c705a3806d97a5\\n',\n",
       " '> completed      : sha256:35c102085707f703de2d9eaad8752d6fe1b8f02b5d2149f1d8357c9cc7fb7d0a\\n',\n",
       " '> completed      : sha256:cf2c808e01bd05576dae1e1b5d81e0c55909082419365c5829b1cc00b927e217\\n',\n",
       " '> completed      : sha256:180be0bebdb64000b9a30b3fbf313fbfbac92dc751a6e5bff68f7fff41d39c31\\n',\n",
       " '> completed      : sha256:24c54dd105b4378e24b228793f3bf8ac2cb5f354e41391f128317c1917191935\\n',\n",
       " '> completed      : sha256:f72d004efc5ffbd672187f45c9e06c60935942fd45e4b09fba4b3bf3ef3f4c93\\n',\n",
       " '> completed      : sha256:f6d009f607640ca352523ff93323f3133877dd4652bd84b53df506828e19c881\\n',\n",
       " '> completed      : sha256:952472aec7fb3f407a404c870120d52f721e9fa15fe4a8db8ded620e4b1a6a2d\\n',\n",
       " '> failed         : sha256:66665379e90bb2ea40c6632650c30f5977510b67de4c8b74aafebda433e8f194\\n',\n",
       " '[1454066.7962521] [nid01143-46418] [Puller] [ERROR] Error trace (most nested error last):\\n',\n",
       " '#0   downloadStream at \"Puller.cpp\":278 Download stream error\\n',\n",
       " '#1   \"unknown function\" at \"unknown file\":-1 Failed to read response body\\n',\n",
       " '> retry          : sha256:66665379e90bb2ea40c6632650c30f5977510b67de4c8b74aafebda433e8f194\\n',\n",
       " '> pulling        : sha256:66665379e90bb2ea40c6632650c30f5977510b67de4c8b74aafebda433e8f194\\n',\n",
       " \"terminate called after throwing an instance of 'std::bad_alloc'\\n\",\n",
       " '  what():  std::bad_alloc\\n',\n",
       " 'srun: error: nid01143: task 0: Aborted\\n',\n",
       " 'srun: Terminating job step 20426338.0\\n',\n",
       " 'Specified image index.docker.io/thevirtualbrain/tvb_converter:latest is not available\\n',\n",
       " 'srun: error: nid01143: task 0: Exited with exit code 1\\n',\n",
       " 'srun: Terminating job step 20426338.1\\n',\n",
       " ' \\n',\n",
       " 'Batch Job Summary Report for Job \"job_script_tvb_conv\" (20426338) on daint\\n',\n",
       " '-----------------------------------------------------------------------------------------------------\\n',\n",
       " '             Submit            Eligible               Start                 End    Elapsed  Timelimit \\n',\n",
       " '------------------- ------------------- ------------------- ------------------- ---------- ---------- \\n',\n",
       " '2020-02-20T08:46:59 2020-02-20T08:46:59 2020-02-20T08:46:59 2020-02-20T08:47:49   00:00:50   01:00:00 \\n',\n",
       " '-----------------------------------------------------------------------------------------------------\\n',\n",
       " 'Username    Account     Partition   NNodes   Energy\\n',\n",
       " '----------  ----------  ----------  ------  --------------\\n',\n",
       " 'bp000225    ich012      normal           1    3.80K joules\\n',\n",
       " ' \\n',\n",
       " 'This job did not utilize any GPUs\\n',\n",
       " ' \\n',\n",
       " '----------------------------------------------------------\\n',\n",
       " 'Scratch File System        Files       Quota\\n',\n",
       " '--------------------  ----------  ----------\\n',\n",
       " '/scratch/snx3000           31222     1000000\\n',\n",
       " ' \\n']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slurm_output = tvb_conv_job.working_dir.stat(\"slurm-\" + job_script_tvb_conv + \".out\").raw().readlines()\n",
    "slurm_output[-40:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Download TVB ready data\n",
    "\n",
    " On the supercomputer, results are stored in the folder\n",
    "```\n",
    "<tvb_output>\n",
    "```\n",
    "\n",
    "If everything went smoothly, there will be nine files in this folder:  \n",
    "\n",
    "```\n",
    "sub-<participant_label>_task-rest_parc-desikan_ROI_timeseries.txt\n",
    "sub-<participant_label>_EEGProjection.mat\n",
    "sub-<participant_label>_region_mapping.txt\n",
    "sub-<participant_label>_inner_skull_surface.zip\n",
    "sub-<participant_label>_Cortex.zip\n",
    "sub-<participant_label>_outer_skull_surface.zip\n",
    "sub-<participant_label>_outer_skin_surface.zip\n",
    "sub-<participant_label>_EEG_Locations.txt\n",
    "sub-<participant_label>_Connectome.zip\n",
    "```\n",
    "\n",
    "We will go on to zip the folder and download the results to the Docker image filesystem in which the Jupyter notebook client is running. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "site_client.execute(\"zip -r \" + tvb_output+\".zip \" + tvb_output)\n",
    "output_hdl = wd_handle.stat(\"/tvb_converter_workdir/TVB_output.zip\")\n",
    "output_hdl.download(\"TVB_output.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All that is left to do is to copy the files from the image's virtual filesystem into Collab Storage where you can download the files by clicking on them and then hitting the \"Download\" button."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'8ff55081-8e05-4c54-bc70-9ef1154efdd0'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collab_storage.upload_file(\"TVB_output.zip\", COLLAB_PATH+\"/TVB_output.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally the created derivatives for TVB are stored in BIDS format inside the dataset provided at the beginning.\n",
    "We will now zip and download this one too. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15. Download pipeline output in BIDS format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline results are also stored according to the BIDS specifications inside the /derivatives/TVB directory of the given BIDS folder. \n",
    "The data are stored following the specifications of the Common derivatives and Computational models BIDS extension proposal. <br><br>\n",
    "\n",
    "Common derivatives <br>\n",
    "https://docs.google.com/document/d/1Wwc4A6Mow4ZPPszDIWfCUCRNstn7d_zzaWPcfcHmgI4/edit <br>\n",
    "<br>\n",
    "Computational models <br>\n",
    "https://docs.google.com/document/d/1oaBWmkrUqH28oQb1PTO-rG_kuwNX9KqAoE9i5iDh1xw/edit#heading=h.mqkmyp254xh6 <br>\n",
    "<br>\n",
    "\n",
    "What remains is to zip, download the BIDS folder from the HPC and to add a additional dataset_description.json to the /derivatives directory to comply with BIDS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# zip the BIDS folder on the HPC\n",
    "site_client.execute(\"cd \" + base_folder + \" && zip -r \" + dataset + \" \" \n",
    "                    + dataset.strip(\".zip\"))\n",
    "\n",
    "# download the BIDS folder\n",
    "output_hdl = wd_handle.stat(\"/\"+dataset)\n",
    "output_hdl.download(dataset)\n",
    "\n",
    "# unzip \n",
    "import zipfile\n",
    "with zipfile.ZipFile(dataset, 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate BIDS-compliant metadata file dataset_description.json\n",
    "\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "# Arguments\n",
    "input_dataset_description_json = dataset.strip(\".zip\")+'/dataset_description.json'\n",
    "output_dataset_description_json = dataset.strip(\".zip\")+'/derivatives/TVB/dataset_description.json'\n",
    "\n",
    "\n",
    "# 1 read the dataset_description.json from the raw data set \n",
    "with open(input_dataset_description_json, \"r\") as input_json:\n",
    "    data = json.load(input_json)\n",
    "\n",
    "\n",
    "# 2 prepend \"TVB pipeline derivative: \" to the value of the key \"Name\" \n",
    "# to indicate that this is not the original raw data set, but\n",
    "# a derivative generated by the TVB pipeline.\n",
    "data['Name'] = \"TVB pipeline derivative: \" + data['Name'] \n",
    "\n",
    "\n",
    "## Uncomment below to generate the data dict from scratch\n",
    "#data= OrderedDict()\n",
    "#data['Name'] = ''\n",
    "#data['BIDSVersion'] = '1.0.2'\n",
    "#data['License'] = ''\n",
    "#data['Authors'] = ['','','']\n",
    "#data['HowToAcknowledge'] = ''\n",
    "#data['Funding'] = ['','','']\n",
    "#data['ReferencesAndLinks'] = ['','','']\n",
    "#data['DatasetDOI'] = ''\n",
    "\n",
    "\n",
    "# 3 Add fields for BIDS derivatives\n",
    "data['PipelineDescription'] = {\n",
    "    \t\"Name\": \"TVB\", # REQUIRED: this field must be a substring of the folder name of the pipeline output\n",
    "        \"Version\": \"1.0\", \n",
    "    \t\"CodeURL\": \"https://github.com/BrainModes\",\n",
    "    \t\"DockerHubContainerTag\": \"thevirtualbrain/tvb_pipeline\",\n",
    "    \t\"SingularityContainerURL\": \"\",\n",
    "    \t\"SingularityContainerVersion\": \"\" \n",
    "    }\n",
    "\n",
    "data['SourceDatasets'] = [\n",
    "    \t{\n",
    "    \t\t\"URL\": \"\",\n",
    "    \t\t\"DOI\": data['DatasetDOI'],\n",
    "    \t\t\"Version\": \"\"\n",
    "    \t}\n",
    "#    \t,{\n",
    "#    \t\t\"URL\": \"\",\n",
    "#    \t\t\"DOI\": \"\",\n",
    "#    \t\t\"Version\": \"\"\n",
    "#    \t}    \n",
    "    ]\n",
    "\n",
    "\n",
    "# 4 Save JSON\n",
    "with open(output_dataset_description_json, 'w') as ff:\n",
    "    json.dump(data, ff,sort_keys=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 20\n",
      "-rw-r--r-- 1 jovyan users  104 Jan 27 09:59 dataset_description.json\n",
      "drwxr-xr-x 4 jovyan users 4096 Jan 27 09:59 \u001b[0m\u001b[01;34mderivatives\u001b[0m/\n",
      "-rw-r--r-- 1 jovyan users   51 Jan 27 09:59 participants.tsv\n",
      "drwxr-xr-x 6 jovyan users 4096 Jan 27 09:59 \u001b[01;34msub-QL20120814\u001b[0m/\n",
      "-rw-r--r-- 1 jovyan users   57 Jan 27 09:59 task-rest_bold.json\n"
     ]
    }
   ],
   "source": [
    "ls -l dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "# add README, CHANGES and participants.tsv to the derivatives folder\n",
    "if os.path.exists(dataset.strip(\".zip\")+\"/README\"):\n",
    "    shutil.copyfile(dataset.strip(\".zip\")+\"/README\", dataset.strip(\".zip\")+\"/derivatives/TVB/README\")\n",
    "if os.path.exists(dataset.strip(\".zip\")+\"/CHANGES\"):\n",
    "    shutil.copyfile(dataset.strip(\".zip\")+\"/CHANGES\", dataset.strip(\".zip\")+\"/derivatives/TVB/CHANGES\")\n",
    "if os.path.exists(dataset.strip(\".zip\")+\"/participants.tsv\"):\n",
    "    shutil.copyfile(dataset.strip(\".zip\")+\"/participants.tsv\", dataset.strip(\".zip\")+\"/derivatives/TVB/participants.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/BIDS_test.zip'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# zip the BIDS folder\n",
    "shutil.make_archive(dataset.strip(\".zip\"), 'zip', dataset.strip(\".zip\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'521f92bb-1c6c-4dd9-907d-8a225ca914ae'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and download to the collab storage\n",
    "collab_storage.upload_file(dataset, COLLAB_PATH+\"/BIDS_TVB_derivatives.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integration with HBP Knowledgegraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To integrate the produced pipeline output into KnowledgeGraph, the user is required to provide values for MINDS metadata keys. The provided metadata is stored in the file minds_metadata.json in order to be read out by the curation team. <br>\n",
    "<br>\n",
    "Please read through the following lines of code and enter applicable information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# generate MINDS-compliant metadata file minds_metadata.json\n",
    "\n",
    "# Arguments\n",
    "output_MINDS_JSON = 'minds_metadata.json'\n",
    "\n",
    "# Pipeline users are requested to fill out the following fields\n",
    "# in order to integrate data with KnowledgeGraph\n",
    "# Enter the information between the \"\"\n",
    "Dataset__ID = \"\" # enter your unique identifier for this metadata block, will later be replaced by a system (Knowledge Graph) wide unique identifier \n",
    "Dataset__has_contributor = \"\" # enter corresponding Person block _ID \n",
    "Dataset_created_as = \"\" # choose: HBP-SGA1, HBP-SGA2, HBP-SGA3, external \n",
    "Dataset__has_custodian = \"\" # enter corresponding Person block _ID \n",
    "Dataset_description = \"\" # describe the content of this metadata block \n",
    "Dataset_DOI = \"\" # if this dataset already has a DOI, please enter it here (otherwise a DOI will be assigned to this dataset when it is released \n",
    "Dataset_embargo_status = \"\" # choose: embargoed, not-embargoed \n",
    "Dataset_intended_release_date = \"\" # if you chose \"embargoed\" please state here the intended release date (format: yyyy-mm-dd) \n",
    "Dataset_license = \"\" # enter the data license, choose (e.g.) from Creative Comments licence list \n",
    "Dataset__has_main_contact = \"\" # enter corresponding Person block _ID \n",
    "Dataset__has_main_file_bundle = \"\" # enter the corresponding FileBundle block _ID \n",
    "Dataset_title = \"\" # enter a meaningful title for this metadata block \n",
    "EthicsApproval__ID = \"\" # enter your unique identifier for this metadata block, will later be replaced by a system (Knowledge Graph) wide unique identifier \n",
    "EthicsApproval_authority = \"\" # enter the ethics authority name of this ethics approval \n",
    "EthicsApproval_country = \"\" # enter the country of this ethics approval was issued \n",
    "EthicsApproval_ID = \"\" # enter the received identifier of this ethics approval \n",
    "EthicsApproval_SP12_approval = \"\" # choose: yes, no \n",
    "File__ID = \"\" # enter your unique identifier for this metadata block, will later be replaced by a system (Knowledge Graph) wide unique identifier \n",
    "File_description = \"\" # describe the content of this metadata block \n",
    "File_format = \"\" # enter format of file of this File block \n",
    "File_title = \"\" # enter a meaningful title for this metadata block \n",
    "File_URL = \"\" # enter URL to file of this File block \n",
    "FileBundle__ID = \"\" # enter your unique identifier for this metadata block, will later be replaced by a system (Knowledge Graph) wide unique identifier \n",
    "FileBundle_description = \"\" # describe the content of this metadata block \n",
    "FileBundle_format = \"\" # enter format of file occurring in this FileBundle block \n",
    "FileBundle_tag = \"\" # enter tag for grouping reason \n",
    "FileBundle_title = \"\" # enter a meaningful title for this metadata block \n",
    "FileBundle_URL = \"\" # enter URL to the file-folder or URLs to corresponding files belonging to this FileBundle block (multiple entries as multiple rows) \n",
    "FundingInformation__ID = \"\" # enter your unique identifier for this metadata block, will later be replaced by a system (Knowledge Graph) wide unique identifier \n",
    "FundingInformation_grant_ID = \"\" # enter identification number of the grant of this FundingInformation block  \n",
    "FundingInformation_name = \"\" # enter name of funding institution of this FundingInformation block  \n",
    "MethodParadigm__ID = \"\" # enter your unique identifier for this metadata block, will later be replaced by a system (Knowledge Graph) wide unique identifier \n",
    "MethodParadigm_abbreviation = \"\" # enter abbreviated name of instance described in this metadata block \n",
    "MethodParadigm_description = \"\" # describe the content of this metadata block \n",
    "MethodParadigm_experimental_type = \"\" # choose: in vivo, ex vivo, in utero, in vitro, in situ, in silico \n",
    "MethodParadigm_full_name = \"\" # enter full name of instance described in this metadata block \n",
    "MethodParadigm_type = \"\" # enter high level type to which the method/paradigm of this Method/Paradigm block belongs to (e.g., imaging, behavioral assay) \n",
    "ModelInstance__ID = \"\" # enter your unique identifier for this metadata block, will later be replaced by a system (Knowledge Graph) wide unique identifier \n",
    "ModelInstance__is_modelling_sub_cellular_target = \"\" # enter corresponding StudyTarget block _ID \n",
    "ModelInstance_abstraction_level = \"\" # choose: protein structure, systems biology, spiking neurons, rate neurons, population modelling, cognitive modelling or enter other abstraction level \n",
    "ModelInstance_alias = \"\" # enter alternative name for this metadata block  \n",
    "ModelInstance__is_modelling_brain_structure = \"\" # enter corresponding StudyTarget block _ID \n",
    "ModelInstance__has_contributor = \"\" # enter corresponding Person block _ID \n",
    "ModelInstance__has_custodian = \"\" # enter corresponding Person block _ID \n",
    "ModelInstance_description = \"\" # describe content of this metadata block \n",
    "ModelInstance__has_main_contact = \"\" # enter corresponding Person block _ID \n",
    "ModelInstance_model_format = \"\" # choose: NeuroML, PyNN, SONATA, NEURON-Python, NEURON-Hoc, NEST-SLI, NEST-PYTHON, Java, C++, C, Brian, NineML, MATLAB, NetPyNE \n",
    "ModelInstance_model_format_version_compatibility = \"\" # enter which model format version is compatible for the model of this ModelInstant block \n",
    "ModelInstance_model_scope = \"\" # choose: subcellular model (spine, ion channel, signalling, or molecular), single cell model, network model (microcircuit, brain region, or whole brain) or enter other model scope \n",
    "ModelInstance_species = \"\" # enter binomial name of species used for this ModelInstant (e.g., Homo sapiens, Mus musculus, Rattus norvegicus, Macaca mulatta, Macaca fascicularis) \n",
    "ModelInstance_title = \"\" # enter a meaningful title for this metadata block \n",
    "ModelInstance_version = \"\" # enter version number for model described in this ModelInstance \n",
    "Person__ID = \"\" # enter your unique identifier for this metadata block, will later be replaced by a system (Knowledge Graph) wide unique identifier \n",
    "Person_email = \"\" # if no ORCID, please enter email of this Person \n",
    "Person_first_name = \"\" # if no ORCID, please enter first name of this Person \n",
    "Person_last_name = \"\" # if no ORCID, please enter last name of this Person \n",
    "Person_ORCID = \"\" # if available, enter ORCID of this Person \n",
    "PLAComponent__ID = \"\" # enter your unique identifier for this metadata block, will later be replaced by a system (Knowledge Graph) wide unique identifier \n",
    "PLAComponent_associated_task_ID = \"\" # enter identifier of associated HBP task for corresponding PLA component \n",
    "PLAComponent_ID = \"\" # enter identifier of corresponding PLA component \n",
    "PLAComponent__has_owner = \"\" # enter corresponding Person block _ID \n",
    "PLAComponent_phase = \"\" # enter HBP project phase of corresponding PLA component (e.g., HBP-SGA1) \n",
    "Project__ID = \"\" # enter your unique identifier for this metadata block, will later be replaced by a system (Knowledge Graph) wide unique identifier \n",
    "Project__has_coordinator = \"\" # enter corresponding Person block_ID \n",
    "Project_description = \"\" # describe the content of this metadata block \n",
    "Project_title = \"\" # enter a meaningful title for this metadata block \n",
    "PublicationResource__ID = \"\" # enter your unique identifier for this metadata block, will later be replaced by a system (Knowledge Graph) wide unique identifier \n",
    "PublicationResource_ID = \"\" # enter identifier of corresponding publication/resource \n",
    "PublicationResource_ID_type = \"\" # choose: DOI, ISSN, ISBN, URL, or other identifier type \n",
    "StudyTarget__ID = \"\" # enter your unique identifier for this metadata block, will later be replaced by a system (Knowledge Graph) wide unique identifier \n",
    "StudyTarget_abbreviation = \"\" # enter abbreviated name of instance described in this metadata block \n",
    "StudyTarget_full_name = \"\" # enter full name of instance described in this metadata block \n",
    "StudyTarget_source_of_name = \"\" # if full name is chosen from a terminology or ontology, enter name of corresponding terminology or ontology list \n",
    "StudyTarget_type = \"\" # choose: disease, disease model, tissue type, brain structure, cell type, subcellular structure, macromolecular complex, biological process \n",
    "Subject__ID = \"\" # enter your unique identifier for this metadata block, will later be replaced by a system (Knowledge Graph) wide unique identifier \n",
    "Subject_age = \"\" # enter age of subject described in this Subject block \n",
    "Subject_age_category = \"\" # choose: embryo, neonate, juvenile, young adult, adult, aged adult \n",
    "Subject_age_range_max = \"\" # within in the time frame of the experiment, enter maximum age of subject described in this Subject block \n",
    "Subject_age_range_min = \"\" # within in the time frame of the experiment, enter minimum age of subject described in this Subject block \n",
    "Subject_alias = \"\" # enter alternative name for this metadata block  \n",
    "Subject_disabilitydisease = \"\" # enter disability, disease or disease model the subject of this Subject block has  \n",
    "Subject_genotype = \"\" # enter genotype of the subject described in this Subject block \n",
    "Subject_handedness = \"\" # choose: left, right, ambidextrous \n",
    "Subject_sex = \"\" # choose: female, male, hermaphrodite \n",
    "Subject_species = \"\" # enter binomial species name of this Subject (e.g., Homo sapiens, Mus musculus, Rattus norvegicus, Macaca mulatta, Macaca fascicularis) \n",
    "Subject_strain = \"\" # enter strain of the subject described in this Subject block \n",
    "SubjectGroup__ID = \"\" # enter your unique identifier for this metadata block, will later be replaced by a system (Knowledge Graph) wide unique identifier \n",
    "SubjectGroup_age_category = \"\" # choose: embryo, neonate, juvenile, young adult, adult, aged adult \n",
    "SubjectGroup_age_range_max = \"\" # within in the time frame of the experiment, enter maximum age of subject described in this Subject block \n",
    "SubjectGroup_age_range_min = \"\" # within in the time frame of the experiment, enter minimum age of subject described in this Subject block \n",
    "SubjectGroup_alias = \"\" # enter alternative name for this metadata block  \n",
    "SubjectGroup_description = \"\" # describe content of this metadata block \n",
    "SubjectGroup_disabilitydisease = \"\" # enter disability, disease or disease model the subjects connected to this SubjectGroup block have  \n",
    "SubjectGroup_genotype = \"\" # enter genotype of subjects connected to this SubjectGroup block \n",
    "SubjectGroup_handedness = \"\" # choose: left, right, ambidextrous \n",
    "SubjectGroup_number_of_subjects = \"\" # enter number of subjects connected to this SubjectGroup block\n",
    "SubjectGroup_sex = \"\" # choose: female, male, hermaphrodite \n",
    "SubjectGroup_species = \"\" # enter binomial name of species occurring in this SubjectGroup (e.g., Homo sapiens, Mus musculus, Rattus norvegicus, Macaca mulatta, Macaca fascicularis) \n",
    "SubjectGroup_strain = \"\" # enter strain of subjects connected to this SubjectGroup block \n",
    "TissueSample__ID = \"\" # enter your unique identifier for this metadata block, will later be replaced by a system (Knowledge Graph) wide unique identifier \n",
    "TissueSample_alias = \"\" # enter alternative name for this metadata block  \n",
    "TissueSample_hemisphere = \"\" # choose: left, right \n",
    "TissueSample_pathology = \"\" # enter pathology state of tissue described in this TissueSample block \n",
    "TissueSample_type = \"\" # choose: whole brain, hemisphere, slice, brain part, cortical column, or enter other tissue sample type \n",
    "\n",
    "##################################################\n",
    "# Create MINDS JSON object\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "\n",
    "data = OrderedDict()\n",
    "\n",
    "data['Dataset'] = [\n",
    "    \t{\n",
    "    \t\t\"_ID\": Dataset__ID,\n",
    "    \t\t\"_has_contributor\": Dataset__has_contributor,\n",
    "    \t\t\"created as\": Dataset_created_as,\n",
    "            \"_has_custodian\": Dataset__has_custodian,\n",
    "    \t\t\"description\": Dataset_description,\n",
    "    \t\t\"DOI\": Dataset_DOI,\n",
    "            \"embargo status\": Dataset_embargo_status,\n",
    "    \t\t\"intended release date\": Dataset_intended_release_date,\n",
    "    \t\t\"license\": Dataset_license,\n",
    "            \"_has_main_contact\": Dataset__has_main_contact,\n",
    "    \t\t\"_has_main_file_bundle\": Dataset__has_main_file_bundle,\n",
    "    \t\t\"title\": Dataset_title\n",
    "    \t}  \n",
    "    ]\n",
    "\n",
    "data['EthicsApproval'] = [\n",
    "    \t{\n",
    "    \t\t\"_ID\": EthicsApproval__ID,\n",
    "    \t\t\"authority\": EthicsApproval_authority,\n",
    "    \t\t\"country\": EthicsApproval_country,\n",
    "            \"ID\": EthicsApproval_ID,\n",
    "    \t\t\"SP12 approval\": EthicsApproval_SP12_approval\n",
    "    \t}  \n",
    "    ]\n",
    "\n",
    "data['File'] = [\n",
    "    \t{\n",
    "    \t\t\"_ID\": File__ID,\n",
    "    \t\t\"description\": File_description,\n",
    "    \t\t\"format\": File_format,\n",
    "            \"title\": File_title,\n",
    "    \t\t\"URL\": File_URL\n",
    "    \t}  \n",
    "    ]\n",
    "\n",
    "data['FileBundle'] = [\n",
    "    \t{\n",
    "    \t\t\"_ID\": FileBundle__ID,\n",
    "    \t\t\"description\": FileBundle_description,\n",
    "    \t\t\"format\": FileBundle_format,\n",
    "            \"tag\": FileBundle_tag,\n",
    "    \t\t\"title\": FileBundle_title,\n",
    "    \t\t\"URL\": FileBundle_URL\n",
    "    \t}  \n",
    "    ]\n",
    "\n",
    "data['FundingInformation'] = [\n",
    "    \t{\n",
    "    \t\t\"_ID\": FundingInformation__ID,\n",
    "    \t\t\"grant ID\": FundingInformation_grant_ID,\n",
    "    \t\t\"name\": FundingInformation_name\n",
    "    \t}  \n",
    "    ]\n",
    "\n",
    "data['Method/Paradigm'] = [\n",
    "    \t{\n",
    "    \t\t\"_ID\": MethodParadigm__ID,\n",
    "    \t\t\"abbreviation\": MethodParadigm_abbreviation,\n",
    "    \t\t\"description\": MethodParadigm_description,\n",
    "            \"experimental type\": MethodParadigm_experimental_type,\n",
    "    \t\t\"full name\": MethodParadigm_full_name,\n",
    "            \"type\": MethodParadigm_type\n",
    "    \t}  \n",
    "    ]\n",
    "\n",
    "data['ModelInstance'] = [\n",
    "    \t{\n",
    "    \t\t\"_ID\": ModelInstance__ID,\n",
    "    \t\t\"_is_modelling_sub_cellular_target\": ModelInstance__is_modelling_sub_cellular_target,\n",
    "    \t\t\"abstraction level\": ModelInstance_abstraction_level,\n",
    "    \t\t\"alias\": ModelInstance_alias,\n",
    "            \"_is_modelling_brain_structure\": ModelInstance__is_modelling_brain_structure,\n",
    "    \t\t\"_has_contributor\": ModelInstance__has_contributor,\n",
    "            \"_has_custodian\": ModelInstance__has_custodian,\n",
    "            \"description\": ModelInstance_description,\n",
    "    \t\t\"_has_main_contact\": ModelInstance__has_main_contact,\n",
    "    \t\t\"model format\": ModelInstance_model_format,\n",
    "    \t\t\"model format version compatibility\": ModelInstance_model_format_version_compatibility,\n",
    "            \"model scope\": ModelInstance_model_scope,\n",
    "    \t\t\"species\":ModelInstance_species,\n",
    "    \t\t\"title\": ModelInstance_title,\n",
    "            \"version\": ModelInstance_version\n",
    "    \t}  \n",
    "    ]\n",
    "\n",
    "\n",
    "data['Person'] = [\n",
    "    \t{\n",
    "    \t\t\"_ID\": Person__ID,\n",
    "    \t\t\"email\": Person_email,\n",
    "    \t\t\"first name\": Person_first_name,\n",
    "            \"last name\": Person_last_name,\n",
    "    \t\t\"ORCID\": Person_ORCID\n",
    "    \t}  \n",
    "    ]\n",
    "\n",
    "data['PLAComponent'] = [\n",
    "    \t{\n",
    "    \t\t\"_ID\": PLAComponent__ID,\n",
    "    \t\t\"associated task (ID)\": PLAComponent_associated_task_ID,\n",
    "    \t\t\"ID\": PLAComponent_ID,\n",
    "            \"_has_owner\": PLAComponent__has_owner,\n",
    "    \t\t\"phase\": PLAComponent_phase\n",
    "    \t}  \n",
    "    ]\n",
    "\n",
    "\n",
    "data['Project'] = [\n",
    "    \t{\n",
    "    \t\t\"_ID\": Project__ID,\n",
    "    \t\t\"_has_coordinator\": Project__has_coordinator,\n",
    "    \t\t\"description\": Project_description,\n",
    "            \"title\": Project_title\n",
    "    \t}  \n",
    "    ]\n",
    "\n",
    "data['Publication/Resource'] = [\n",
    "    \t{\n",
    "    \t\t\"_ID\": PublicationResource__ID,\n",
    "    \t\t\"ID\": PublicationResource_ID,\n",
    "    \t\t\"ID type\": PublicationResource_ID_type\n",
    "    \t}  \n",
    "    ]\n",
    "\n",
    "\n",
    "data['StudyTarget'] = [\n",
    "    \t{\n",
    "    \t\t\"_ID\": StudyTarget__ID,\n",
    "    \t\t\"abbreviation\": StudyTarget_abbreviation,\n",
    "    \t\t\"full name\": StudyTarget_full_name,\n",
    "            \"source of name\": StudyTarget_source_of_name,\n",
    "    \t\t\"type\": StudyTarget_type\n",
    "    \t}  \n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "data['Subject'] = [\n",
    "    \t{\n",
    "    \t\t\"_ID\": Subject__ID,\n",
    "    \t\t\"age\": Subject_age,\n",
    "    \t\t\"age category\": Subject_age_category,\n",
    "    \t\t\"age range (max)\": Subject_age_range_max,\n",
    "            \"age range (min)\": Subject_age_range_min,\n",
    "    \t\t\"alias\": Subject_alias,\n",
    "            \"disability/disease\": Subject_disabilitydisease,\n",
    "            \"genotype\": Subject_genotype,\n",
    "    \t\t\"handedness\": Subject_handedness,\n",
    "    \t\t\"sex\": Subject_sex,\n",
    "    \t\t\"species\": Subject_species,\n",
    "            \"strain\": Subject_strain\n",
    "    \t}  \n",
    "    ]\n",
    "\n",
    "data['SubjectGroup'] = [\n",
    "    \t{\n",
    "    \t\t\"_ID\": SubjectGroup__ID,\n",
    "    \t\t\"age category\": SubjectGroup_age_category,\n",
    "    \t\t\"age range (max)\": SubjectGroup_age_range_max,\n",
    "            \"age range (min)\": SubjectGroup_age_range_min,\n",
    "    \t\t\"alias\": SubjectGroup_alias,\n",
    "    \t\t\"description\": SubjectGroup_description,\n",
    "            \"disability/disease\": SubjectGroup_disabilitydisease,\n",
    "            \"genotype\": SubjectGroup_genotype,\n",
    "    \t\t\"handedness\": SubjectGroup_handedness,\n",
    "    \t\t\"number of subjects\": SubjectGroup_handedness,\n",
    "    \t\t\"sex\": SubjectGroup_sex,\n",
    "    \t\t\"species\": SubjectGroup_species,\n",
    "            \"strain\": SubjectGroup_strain\n",
    "    \t}  \n",
    "    ]\n",
    "\n",
    "data['TissueSample'] = [\n",
    "    \t{\n",
    "    \t\t\"_ID\": TissueSample__ID,\n",
    "    \t\t\"alias\": TissueSample_alias,\n",
    "    \t\t\"hemisphere\": TissueSample_hemisphere,\n",
    "            \"pathology\": TissueSample_pathology,\n",
    "            \"type\": TissueSample_type\n",
    "    \t}  \n",
    "    ]\n",
    "\n",
    "\n",
    "# Save JSON\n",
    "with open(output_MINDS_JSON, 'w') as ff:\n",
    "    json.dump(data, ff,sort_keys=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'70ce6cd9-4b4c-4f85-9006-8e07a9b4cfef'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the minds_metadata.json to HBP Collab storage\n",
    "collab_storage.upload_file(output_MINDS_JSON, COLLAB_PATH+\"/\"+output_MINDS_JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
